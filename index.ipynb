{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%pip install -r requirments.txt --force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1IbRSNeSAZBm_6oszKSRzxABp-PbF9rTF#scrollTo=USsJS0U-LilJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask_socketio import SocketIO, emit, join_room, leave_room\n",
    "from flask import *\n",
    "from flask_login import *\n",
    "from flask_session import Session\n",
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# from datetime import timedelta\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:5100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "app.secret_key = \"mkqw2o0@#mk12!mk3\"\n",
    "host = 'localhost'\n",
    "port = 5100\n",
    "\n",
    "socketio = SocketIO(app)\n",
    "lm = LoginManager()\n",
    "lm.init_app(app)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs = {'device': device}, # 모델이 CPU에서 실행되도록 설정. GPU를 사용할 수 있는 환경이라면 'cuda'로 설정할 수도 있음\n",
    "    encode_kwargs = {'normalize_embeddings': True}, # 임베딩 정규화. 모든 벡터가 같은 범위의 값을 갖도록 함. 유사도 계산 시 일관성을 높여줌\n",
    ")\n",
    "\n",
    "ngrok = ''\n",
    "if ngrok != '':\n",
    "    #Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\n",
    "    #https://huggingface.co/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF/tree/main\n",
    "    llm_model = ChatOllama(\n",
    "        model='meta-llama-3.1',\n",
    "        num_predict=256,\n",
    "        base_url=ngrok\n",
    "    )\n",
    "else:\n",
    "    llm_model = ChatOllama(\n",
    "        model='meta-llama-3.1',\n",
    "        num_predict=256,\n",
    "    )\n",
    "\n",
    "prompt_template = '''Use the following pieces of context to answer the question at the end.\n",
    "If you don't find the answer in context, just say that '내용을 확인할 수 없습니다.', don't try to make up an answer.\n",
    "If you find the answer in context, answer me only use korean.\n",
    "\n",
    "context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:'''\n",
    "rag_prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "@lm.user_loader\n",
    "def user_loader(userId):\n",
    "    userInfo = User.get_user_info(userId)\n",
    "    return User(userInfo)\n",
    "\n",
    "@lm.unauthorized_handler\n",
    "def unauthorized():\n",
    "    return redirect('/')\n",
    "\n",
    "#region CLASS\n",
    "\n",
    "class User(UserMixin):\n",
    "    def __init__(self, info):\n",
    "        self.info = info\n",
    "    \n",
    "    #region getter\n",
    "\n",
    "    def get_id(self):\n",
    "        return self.info['userId']\n",
    "\n",
    "    #endregion\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_user_info(userId):\n",
    "        return {'userId': userId}\n",
    "\n",
    "#endregion CLASS\n",
    "\n",
    "#region route\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/chatroom', methods = ['post'])\n",
    "def chatroom():\n",
    "    params = request.form\n",
    "    userId = params['userId']\n",
    "    roomId = params['roomId']\n",
    "    \n",
    "    session['userId'] = userId\n",
    "    session['roomId'] = roomId\n",
    "    \n",
    "    return render_template(\n",
    "        'chat.html',\n",
    "        userId = userId,\n",
    "        roomId = roomId\n",
    "    )\n",
    "\n",
    "@app.route('/chatbot')\n",
    "def chatbot():\n",
    "    if 'processed_pdf' in session:\n",
    "        return 'test msg'\n",
    "    return render_template('RAG.html')\n",
    "\n",
    "@app.route('/login', methods = ['post'])\n",
    "def login():\n",
    "    params = request.get_json()\n",
    "    userId = params['userId']\n",
    "    \n",
    "    userInfo = User.get_user_info(userId)\n",
    "    login_user(User(userInfo))\n",
    "    return jsonify({'result': 1})\n",
    "\n",
    "@app.route('/logout')\n",
    "def logout():\n",
    "    logout_user()\n",
    "    return redirect('/')\n",
    "\n",
    "@app.route('/pdfprocess', methods=['post'])\n",
    "def pdfprocess():\n",
    "    files = [request.files[i] for i in request.files]\n",
    "    \n",
    "    text_sum = ''\n",
    "    \n",
    "    for file in files:\n",
    "        reader = PdfReader(file)\n",
    "        for page in reader.pages: #페이지 별로 텍스트 추출\n",
    "            text = page.extract_text()\n",
    "            corrected_text = text.encode('utf-8', errors='ignore').decode('utf-8') #인코딩 오류 무시 및 텍스트 누적\n",
    "            text_sum += corrected_text +'\\n'\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_text(text_sum)\n",
    "    vectorstore = FAISS.from_texts(\n",
    "        splits,\n",
    "        embedding = embedding_model\n",
    "    )\n",
    "    \n",
    "    vectorstore.save_local('test')\n",
    "    return jsonify({'result': 1})\n",
    "\n",
    "@app.route('/question', methods=['POST'])\n",
    "def question():\n",
    "    vectorstore = FAISS.load_local(\n",
    "        'test',\n",
    "        embedding_model,\n",
    "        allow_dangerous_deserialization=True,\n",
    "    )\n",
    "    params = request.get_json()\n",
    "    question = params['question']\n",
    "    \n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key='chat_history',\n",
    "        return_messages=True,\n",
    "    )\n",
    "\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm_model,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        condense_question_prompt=rag_prompt,\n",
    "        memory=memory,\n",
    "    )\n",
    "    res = conversation_chain({'question': question})\n",
    "    # print(res['chat_history'][1].content)\n",
    "    return jsonify({'result': res['chat_history'][1].content})\n",
    "\n",
    "#endregion route\n",
    "\n",
    "#region socket\n",
    "\n",
    "@socketio.on('joined', namespace = '/chatroom')\n",
    "def chat_joined(d):\n",
    "    roomId = session.get('roomId')\n",
    "    join_room(roomId)\n",
    "\n",
    "@socketio.on('msg', namespace = '/chatroom')\n",
    "def socket_msg(d):\n",
    "    roomId = session.get('roomId')\n",
    "    socketio.emit('msg', d, namespace = '/chatroom', room = roomId)\n",
    "\n",
    "#endregion socket\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(f'http://{host}:{port}')\n",
    "    socketio.run(app = app, host = host, port = port, allow_unsafe_werkzeug = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
