{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import ollama\n",
    "\n",
    "res = ollama.chat(model='llama3-ko', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': '코사인의 미분 공식에 대해 설명해줘'\n",
    "    }\n",
    "])\n",
    "print(res['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from docx import Document\n",
    "import torch\n",
    "import json\n",
    "import faiss\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "ngrok = 'https://48fe-34-168-96-187.ngrok-free.app'\n",
    "llm_model = ChatOllama(\n",
    "    model='meta-llama-3.1',\n",
    "    # num_predict=256,\n",
    "    format='json',\n",
    "    base_url=ngrok\n",
    "    \n",
    ")\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs = {'device': device}, # 모델이 CPU에서 실행되도록 설정. GPU를 사용할 수 있는 환경이라면 'cuda'로 설정할 수도 있음\n",
    "    encode_kwargs = {'normalize_embeddings': True}, # 임베딩 정규화. 모든 벡터가 같은 범위의 값을 갖도록 함. 유사도 계산 시 일관성을 높여줌\n",
    ")\n",
    "\n",
    "\n",
    "prompt_template = '''Use the following pieces of context to answer the question at the end.\n",
    "If you don't find the answer in context, just say that '내용을 확인할 수 없습니다.', don't try to make up an answer.\n",
    "If you find the answer in context, answer me only use korean.\n",
    "\n",
    "context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:'''\n",
    "rag_prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "vectorstore = FAISS.load_local(\n",
    "    'test',\n",
    "    embedding_model,\n",
    "    allow_dangerous_deserialization=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '미국 철강 제조업 수출', 'content': ['제1장: 미국 철강 제조업의 현황과 전망', '제2장: 미국 철강 제조업의 수출 동향', '제3장: 미국 철강 제조업의 수출 시장 분석', '제4장: 미국 철강 제조업의 수출 경쟁력 강화 방안']}\n",
      "{'text': '미국 철강 제조업은 2020년 2분기 기준으로 1,400만 톤의 생산량을 기록했으며, 이는 전년 동기보다 3.4% 감소했다. 미국 철강 제조업의 주요 업종은 강철 및 연합 강철, 스테인리스 강철, 알루미늄 및 기타 금속 제품이다. 미국 철강 제조업의 생산량은 2020년 2분기 기준으로 1,400만 톤을 기록했으며, 이는 전년 동기보다 3.4% 감소했다. 미국 철강 제조업의 주요 업종은 강철 및 연합 강철, 스테인리스 강철, 알루미늄 및 기타 금속 제품이다.'}\n",
      "{'text': '미국 철강 제조업은 2020년 1분기부터 3분기까지 총 4분기에 걸쳐서 수출이 감소했다. 이는 코로나19 범유행으로 인해 세계 경제가 침체되었기 때문이다. 미국의 주요 철강 수출국가는 중국, 캐나다, 멕시코, 일본, 독일 등이다. 미국은 철강을 수출하는 데 있어 중국에 가장 큰 시장이다. 2020년 1분기에 미국이 중국으로 철강을 수출한 금액은 2억 5천만 달러였다. 그 다음으로는 캐나다로 철강을 수출한 금액이 1억 8천만 달러, 멕시코로 철강을 수출한 금액이 1억 4천만 달러, 일본으로 철강을 수출한 금액이 7천만 달러, 독일로 철강을 수출한 금액이 5천만 달러였다. 미국의 철강 수출은 코로나19 범유행 이후에도 꾸준히 증가하고 있다. 2020년 4분기에 미국이 중국으로 철강을 수출한 금액은 3억 2천만 달러, 캐나다로 철강을 수출한 금액은 1억 9천만 달러, 멕시코로 철강을 수출한 금액은 1억 5천만 달러, 일본으로 철강을 수출한 금액은 8천만 달러, 독일로 철강을 수출한 금액은 6천만 달러였다. 미국의 철강 수출은 코로나19 범유행 이후에도 꾸준히 증가하고 있다.'}\n",
      "{'text': '미국 철강 제조업의 수출 시장은 세계적인 수요가 높은 지역으로 알려져 있습니다. 미국은 철강 제조업의 주요 생산국 중 하나이며, 다양한 종류의 철강 제품을 수출하고 있습니다. 미국의 철강 제조업은 고부가 가치 제품을 생산하는 것으로 알려져 있으며, 이러한 제품은 세계적인 수요가 높은 지역으로 수출되고 있습니다.'}\n",
      "{'text': '미국 철강 제조업의 수출 경쟁력을 강화하기 위한 방안은 다음과 같습니다.\\n\\n1. 기술 개발 및 투자: 미국 철강 제조업이 최신 기술을 개발하고 투자를 통해 생산성을 높이고, 제품 품질을 개선하여 국제 시장에서 경쟁력을 확보할 수 있습니다.\\n\\n2. 수출 지원 프로그램: 정부가 수출 지원 프로그램을 제공하여 미국 철강 제조업에 대한 수출 촉진을 도와줄 수 있습니다.\\n\\n3. 국제 협력: 미국 철강 제조업이 국제 협력과 파트너십을 통해 기술 개발, 시장 진출, 그리고 경쟁력을 강화할 수 있습니다.\\n\\n4. 교육 및 훈련: 미국 철강 제조업의 근로자들이 최신 기술과 생산 방법에 대한 교육 및 훈련을 받도록 지원하여 생산성을 높이고, 제품 품질을 개선할 수 있습니다.\\n\\n5. 환경 친화성: 미국 철강 제조업이 환경 친화적인 생산 방법을 개발하고 적용하여 국제 시장에서 경쟁력을 확보할 수 있습니다.\\n\\n6. 안전성: 미국 철강 제조업이 제품의 안전성을 강화하여 국제 시장에서 경쟁력을 확보할 수 있습니다.\\n\\n7. 품질 관리: 미국 철강 제조업이 제품 품질을 관리하고 개선하여 국제 시장에서 경쟁력을 확보할 수 있습니다.\\n\\n8. 마케팅 및 판매: 미국 철강 제조업이 국제 시장에서 제품을 판매하기 위한 마케팅 및 판매 전략을 개발하여 경쟁력을 강화할 수 있습니다.'}\n"
     ]
    }
   ],
   "source": [
    "doc = Document()\n",
    "question = '미국 철강 제조업 수출에 대한 보고서를 만들어줘 `title`: str(보고서의 제목), `content`: list [보고서의 목차별 제목] 20글자 이내로 Respond using JSON only.'\n",
    "# question = '국민건강보험법에서 직장가입자를 구분하는 기준에 대해서 보고서를 만들어줘 `title`: str(보고서의 제목), `content`: list [보고서의 목차별 제목] 20글자 이내로 Respond using JSON only.'\n",
    "# question = ' text`: str(`1. Google AIStudio를 검색한 후 아래 사이트로 접속하고 로그인 -> ‘Gemini API 키 가져오기’ 클릭`에 대한 상세한 정보) resonse in JSON format. only use korean in answer'\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm_model,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    condense_question_prompt=rag_prompt,\n",
    "    memory=memory,\n",
    ")\n",
    "res = conversation_chain({'question': question})\n",
    "# print(res['chat_history'][1].content)\n",
    "response = res['chat_history'][1].content.replace('\\n', '').lstrip().rstrip()\n",
    "response = json.loads(response)\n",
    "print(response)\n",
    "doc.add_heading(response['title'])\n",
    "\n",
    "for cont in response['content']:\n",
    "    doc.add_paragraph(cont)\n",
    "\n",
    "for cont in response['content']:\n",
    "    question = f'`{cont}`에 대한 설명. key is `text`. Respond using JSON only.'\n",
    "\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key='chat_history',\n",
    "        return_messages=True,\n",
    "    )\n",
    "\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm_model,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        condense_question_prompt=rag_prompt,\n",
    "        memory=memory,\n",
    "    )\n",
    "    \n",
    "    res = conversation_chain({'question': question})\n",
    "    # print(res['chat_history'][1].content)\n",
    "    response = res['chat_history'][1].content.replace('\\n', '').lstrip().rstrip()\n",
    "    response = json.loads(response)\n",
    "    print(response)\n",
    "    doc.add_heading(cont, level = 2)\n",
    "    doc.add_paragraph(response['text'])\n",
    "    \n",
    "doc.save('test.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install psycopg2\n",
    "import psycopg2\n",
    "from psycopg2 import pool\n",
    "\n",
    "db = psycopg2.connect(\n",
    "    user='postgres.vpcdvbdktvvzrvjfyyzm',\n",
    "    password='Odvv8E1iChKjwai4',\n",
    "    host='aws-0-ap-southeast-1.pooler.supabase.com',\n",
    "    port=6543,\n",
    "    dbname='postgres'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = psycopg2.connect(\n",
    "    user='moai',\n",
    "    password='smhrd1234',\n",
    "    host='project-db-campus.smhrd.com',\n",
    "    port=3310,\n",
    "    dbname='moai'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain_postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_postgres.vectorstores import PGVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs = {'device': device}, # 모델이 CPU에서 실행되도록 설정. GPU를 사용할 수 있는 환경이라면 'cuda'로 설정할 수도 있음\n",
    "    encode_kwargs = {'normalize_embeddings': True}, # 임베딩 정규화. 모든 벡터가 같은 범위의 값을 갖도록 함. 유사도 계산 시 일관성을 높여줌\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = PGVector(\n",
    "    embeddings=embedding_model,\n",
    "    collection_name='test_docs',\n",
    "    connection='postgresql+psycopg2://postgres.vpcdvbdktvvzrvjfyyzm:Odvv8E1iChKjwai4@aws-0-ap-southeast-1.pooler.supabase.com:6543/postgres',\n",
    "    use_jsonb=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs.delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid.uuid4().hex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_sum = ''\n",
    "file = './03.조건문.pdf'\n",
    "\n",
    "reader = PdfReader(file)\n",
    "for page in reader.pages: #페이지 별로 텍스트 추출\n",
    "    text = page.extract_text()\n",
    "    corrected_text = text.encode('utf-8', errors='ignore').decode('utf-8') #인코딩 오류 무시 및 텍스트 누적\n",
    "    text_sum += corrected_text +'\\n'\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_text(text_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs.add_texts(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
