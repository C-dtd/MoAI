{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import ollama\n",
    "\n",
    "res = ollama.chat(model='llama3-ko', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': '코사인의 미분 공식에 대해 설명해줘'\n",
    "    }\n",
    "])\n",
    "print(res['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from docx import Document\n",
    "import torch\n",
    "import json\n",
    "import faiss\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "ngrok = 'https://48fe-34-168-96-187.ngrok-free.app'\n",
    "llm_model = ChatOllama(\n",
    "    model='meta-llama-3.1',\n",
    "    # num_predict=256,\n",
    "    format='json',\n",
    "    base_url=ngrok\n",
    "    \n",
    ")\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs = {'device': device}, # 모델이 CPU에서 실행되도록 설정. GPU를 사용할 수 있는 환경이라면 'cuda'로 설정할 수도 있음\n",
    "    encode_kwargs = {'normalize_embeddings': True}, # 임베딩 정규화. 모든 벡터가 같은 범위의 값을 갖도록 함. 유사도 계산 시 일관성을 높여줌\n",
    ")\n",
    "\n",
    "\n",
    "prompt_template = '''Use the following pieces of context to answer the question at the end.\n",
    "If you don't find the answer in context, just say that '내용을 확인할 수 없습니다.', don't try to make up an answer.\n",
    "If you find the answer in context, answer me only use korean.\n",
    "\n",
    "context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:'''\n",
    "rag_prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "vectorstore = FAISS.load_local(\n",
    "    'test',\n",
    "    embedding_model,\n",
    "    allow_dangerous_deserialization=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '미국 철강 제조업 수출', 'content': ['제1장: 미국 철강 제조업의 현황과 전망', '제2장: 미국 철강 제조업의 수출 동향', '제3장: 미국 철강 제조업의 수출 시장 분석', '제4장: 미국 철강 제조업의 수출 경쟁력 강화 방안']}\n",
      "{'text': '미국 철강 제조업은 2020년 4분기 기준으로 2,300만 톤의 생산량을 기록했으며, 이는 전년 동기보다 1.3% 감소했다. 미국 철강 제조업의 주요 업종은 강철 및 강재, 열연 및 연마, 기계 및 부품, 건설 및 시공 등이다. 미국 철강 제조업의 생산량은 2020년 4분기 기준으로 2,300만 톤을 기록했으며, 이는 전년 동기보다 1.3% 감소했다. 미국 철강 제조업의 주요 업종은 강철 및 강재, 열연 및 연마, 기계 및 부품, 건설 및 시공 등이다.'}\n",
      "{'text': '미국은 2019년부터 2020년까지 철강 제조업의 수출이 증가했다. 이 기간 동안 미국의 철강 제조업 수출액은 1,430억 달러에서 1,540억 달러로 증가했다. 이는 미국의 철강 제조업이 글로벌 시장에서 점유율을 높이는 데 성공한 결과이다. 미국의 철강 제조업 수출의 증가에는 여러 요인이 작용했는데, 가장 큰 요인은 중국과 유럽의 철강 수요가 증가한 때문이다. 중국은 2019년부터 2020년까지 철강 수요가 10% 이상 증가했고, 유럽도 같은 기간 동안 철강 수요가 5% 이상 증가했다. 미국의 철강 제조업이 이러한 지역에서 수출을 늘린 것은 글로벌 시장에서 점유율을 높이는 데 성공한 결과이다. 또한, 미국의 철강 제조업은 기술 개발과 생산 효율성 향상을 통해 경쟁력을 강화하고 있다. 이는 미국의 철강 제조업이 글로벌 시장에서 더 큰 역할을 할 수 있도록 하는데 기여했다.'}\n",
      "{'text': '미국 철강 제조업의 수출 시장은 세계적인 수요가 높은 지역으로 알려져 있습니다. 미국은 철강 제조업의 주요 생산국 중 하나이며, 수출을 통해 국가 경제에 기여하고 있습니다. 그러나 수출 시장은 경쟁이 치열하며, 가격 및 품질 competitiveness를 유지하는 것이 중요합니다.'}\n",
      "{'text': '미국 철강 제조업의 수출 경쟁력을 강화하기 위한 방안은 다음과 같습니다.\\n\\n1. 기술 개발 및 투자: 미국 철강 제조업이 최신 기술을 개발하고 투자를 통해 생산성을 높이고, 제품 품질을 개선하여 국제 시장에서 경쟁력을 확보할 수 있습니다.\\n\\n2. 수출 지원 프로그램: 정부가 수출 지원 프로그램을 제공하여 미국 철강 제조업에 대한 수출 촉진을 도와줄 수 있습니다.\\n\\n3. 국제 협력: 미국 철강 제조업이 국제 협력과 파트너십을 통해 기술 개발, 시장 진출, 그리고 경쟁력을 강화할 수 있습니다.\\n\\n4. 교육 및 훈련: 미국 철강 제조업의 근로자들이 최신 기술과 생산 방법에 대한 교육 및 훈련을 받도록 지원하여 생산성을 높이고, 제품 품질을 개선할 수 있습니다.\\n\\n5. 환경 친화성: 미국 철강 제조업이 환경 친화적인 생산 방법을 개발하고 적용하여 국제 시장에서 경쟁력을 확보할 수 있습니다.\\n\\n6. 안전성: 미국 철강 제조업이 제품의 안전성을 강화하여 국제 시장에서 경쟁력을 확보할 수 있습니다.\\n\\n7. 품질 관리: 미국 철강 제조업이 제품 품질을 관리하고 개선하여 국제 시장에서 경쟁력을 확보할 수 있습니다.\\n\\n8. 마케팅 및 판매: 미국 철강 제조업이 국제 시장에서 제품을 판매하기 위한 마케팅 및 판매 전략을 개발하여 경쟁력을 강화할 수 있습니다.'}\n"
     ]
    }
   ],
   "source": [
    "doc = Document()\n",
    "question = '미국 철강 제조업 수출에 대한 보고서를 만들어줘 `title`: str(보고서의 제목), `content`: list [보고서의 목차별 제목] 20글자 이내로 Respond using JSON only.'\n",
    "# question = '국민건강보험법에서 직장가입자를 구분하는 기준에 대해서 보고서를 만들어줘 `title`: str(보고서의 제목), `content`: list [보고서의 목차별 제목] 20글자 이내로 Respond using JSON only.'\n",
    "# question = ' text`: str(`1. Google AIStudio를 검색한 후 아래 사이트로 접속하고 로그인 -> ‘Gemini API 키 가져오기’ 클릭`에 대한 상세한 정보) resonse in JSON format. only use korean in answer'\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm_model,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    condense_question_prompt=rag_prompt,\n",
    "    memory=memory,\n",
    ")\n",
    "res = conversation_chain({'question': question})\n",
    "# print(res['chat_history'][1].content)\n",
    "response = res['chat_history'][1].content.replace('\\n', '').lstrip().rstrip()\n",
    "response = json.loads(response)\n",
    "print(response)\n",
    "doc.add_heading(response['title'])\n",
    "\n",
    "for cont in response['content']:\n",
    "    doc.add_paragraph(cont)\n",
    "\n",
    "for cont in response['content']:\n",
    "    question = f'`{cont}`에 대한 설명. key is `text`. Respond using JSON only.'\n",
    "\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key='chat_history',\n",
    "        return_messages=True,\n",
    "    )\n",
    "\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm_model,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        condense_question_prompt=rag_prompt,\n",
    "        memory=memory,\n",
    "    )\n",
    "    \n",
    "    res = conversation_chain({'question': question})\n",
    "    # print(res['chat_history'][1].content)\n",
    "    response = res['chat_history'][1].content.replace('\\n', '').lstrip().rstrip()\n",
    "    response = json.loads(response)\n",
    "    print(response)\n",
    "    doc.add_heading(cont, level = 2)\n",
    "    doc.add_paragraph(response['text'])\n",
    "    \n",
    "doc.save('test.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install psycopg2\n",
    "import psycopg2\n",
    "from psycopg2 import pool\n",
    "\n",
    "db = psycopg2.connect(\n",
    "    user='postgres.vpcdvbdktvvzrvjfyyzm',\n",
    "    password='Odvv8E1iChKjwai4',\n",
    "    host='aws-0-ap-southeast-1.pooler.supabase.com',\n",
    "    port=6543,\n",
    "    dbname='postgres'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = psycopg2.connect(\n",
    "    user='moai',\n",
    "    password='smhrd1234',\n",
    "    host='project-db-campus.smhrd.com',\n",
    "    port=3310,\n",
    "    dbname='moai'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain_postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_postgres.vectorstores import PGVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs = {'device': device}, # 모델이 CPU에서 실행되도록 설정. GPU를 사용할 수 있는 환경이라면 'cuda'로 설정할 수도 있음\n",
    "    encode_kwargs = {'normalize_embeddings': True}, # 임베딩 정규화. 모든 벡터가 같은 범위의 값을 갖도록 함. 유사도 계산 시 일관성을 높여줌\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = PGVector(\n",
    "    embeddings=embedding_model,\n",
    "    collection_name='test_docs',\n",
    "    connection='postgresql+psycopg2://postgres.vpcdvbdktvvzrvjfyyzm:Odvv8E1iChKjwai4@aws-0-ap-southeast-1.pooler.supabase.com:6543/postgres',\n",
    "    use_jsonb=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs.delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid.uuid4().hex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_sum = ''\n",
    "file = './03.조건문.pdf'\n",
    "\n",
    "reader = PdfReader(file)\n",
    "for page in reader.pages: #페이지 별로 텍스트 추출\n",
    "    text = page.extract_text()\n",
    "    corrected_text = text.encode('utf-8', errors='ignore').decode('utf-8') #인코딩 오류 무시 및 텍스트 누적\n",
    "    text_sum += corrected_text +'\\n'\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_text(text_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs.add_texts(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
