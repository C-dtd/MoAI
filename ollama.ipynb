{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import ollama\n",
    "\n",
    "res = ollama.chat(model='llama3-ko', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': '코사인의 미분 공식에 대해 설명해줘'\n",
    "    }\n",
    "])\n",
    "print(res['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from docx import Document\n",
    "import torch\n",
    "import json\n",
    "import faiss\n",
    "from langchain.vectorstores import FAISS\n",
    "# from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "ngrok = 'https://7505-34-125-71-39.ngrok-free.app'\n",
    "llm_model_json = ChatOllama(\n",
    "    model='meta-llama-3.1',\n",
    "    # num_predict=256,\n",
    "    format='json',\n",
    "    base_url=ngrok\n",
    ")\n",
    "llm_model = ChatO\n",
    "llama(\n",
    "    model='meta-llama-3.1',\n",
    "    # num_predict=256,\n",
    "    # format='json',\n",
    "    base_url=ngrok\n",
    ")\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs = {'device': device}, # 모델이 CPU에서 실행되도록 설정. GPU를 사용할 수 있는 환경이라면 'cuda'로 설정할 수도 있음\n",
    "    encode_kwargs = {'normalize_embeddings': True}, # 임베딩 정규화. 모든 벡터가 같은 범위의 값을 갖도록 함. 유사도 계산 시 일관성을 높여줌\n",
    ")\n",
    "\n",
    "\n",
    "prompt_template = '''Use the following pieces of context to answer the question at the end.\n",
    "If you don't find the answer in context, don't try to make up an answer.\n",
    "If you find the answer in context, answer me only use korean.\n",
    "\n",
    "context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:'''\n",
    "rag_prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# vectorstore = FAISS.load_local(\n",
    "#     'test',\n",
    "#     embedding_model,\n",
    "#     allow_dangerous_deserialization=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shoe industry report\n"
     ]
    }
   ],
   "source": [
    "ans = llm_model.invoke('translate to english this sentence, \"신발 산업에 대한 보고서\". answer me only translated sentence.')\n",
    "print(ans.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "신발 산업 보고서\n"
     ]
    }
   ],
   "source": [
    "ans = llm_model.invoke('translate to korean this sentence, \"The shoe industry report\". answer me only translated sentence.')\n",
    "print(ans.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 22\u001b[0m\n\u001b[0;32m     11\u001b[0m memory \u001b[38;5;241m=\u001b[39m ConversationBufferMemory(\n\u001b[0;32m     12\u001b[0m     memory_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     13\u001b[0m     return_messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     16\u001b[0m conversation_chain \u001b[38;5;241m=\u001b[39m ConversationalRetrievalChain\u001b[38;5;241m.\u001b[39mfrom_llm(\n\u001b[0;32m     17\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm_model_json,\n\u001b[0;32m     18\u001b[0m     retriever\u001b[38;5;241m=\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39mas_retriever(),\n\u001b[0;32m     19\u001b[0m     condense_question_prompt\u001b[38;5;241m=\u001b[39mrag_prompt,\n\u001b[0;32m     20\u001b[0m     memory\u001b[38;5;241m=\u001b[39mmemory,\n\u001b[0;32m     21\u001b[0m )\n\u001b[1;32m---> 22\u001b[0m res \u001b[38;5;241m=\u001b[39m conversation_chain({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m: question})\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# print(res['chat_history'][1].content)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m response \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mlstrip()\u001b[38;5;241m.\u001b[39mrstrip()\n",
      "File \u001b[1;32mc:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     emit_warning()\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:383\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    376\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    381\u001b[0m }\n\u001b[1;32m--> 383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m    384\u001b[0m     inputs,\n\u001b[0;32m    385\u001b[0m     cast(RunnableConfig, {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}),\n\u001b[0;32m    386\u001b[0m     return_only_outputs\u001b[38;5;241m=\u001b[39mreturn_only_outputs,\n\u001b[0;32m    387\u001b[0m     include_run_info\u001b[38;5;241m=\u001b[39minclude_run_info,\n\u001b[0;32m    388\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:166\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    161\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    162\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\conversational_retrieval\\base.py:160\u001b[0m, in \u001b[0;36mBaseConversationalRetrievalChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    156\u001b[0m accepts_run_manager \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_docs)\u001b[38;5;241m.\u001b[39mparameters\n\u001b[0;32m    158\u001b[0m )\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accepts_run_manager:\n\u001b[1;32m--> 160\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_docs(new_question, inputs, run_manager\u001b[38;5;241m=\u001b[39m_run_manager)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_docs(new_question, inputs)  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\conversational_retrieval\\base.py:397\u001b[0m, in \u001b[0;36mConversationalRetrievalChain._get_docs\u001b[1;34m(self, question, inputs, run_manager)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_docs\u001b[39m(\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    391\u001b[0m     question: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m     run_manager: CallbackManagerForChainRun,\n\u001b[0;32m    395\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m    396\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get docs.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 397\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretriever\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m    398\u001b[0m         question, config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_manager\u001b[38;5;241m.\u001b[39mget_child()}\n\u001b[0;32m    399\u001b[0m     )\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reduce_tokens_below_limit(docs)\n",
      "File \u001b[1;32mc:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\retrievers.py:221\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    220\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_error(e)\n\u001b[1;32m--> 221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_end(\n\u001b[0;32m    224\u001b[0m         result,\n\u001b[0;32m    225\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\retrievers.py:214\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[1;32m--> 214\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;28minput\u001b[39m, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs\n\u001b[0;32m    216\u001b[0m     )\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:1248\u001b[0m, in \u001b[0;36mVectorStoreRetriever._get_relevant_documents\u001b[1;34m(self, query, run_manager)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_relevant_documents\u001b[39m(\n\u001b[0;32m   1245\u001b[0m     \u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m, run_manager: CallbackManagerForRetrieverRun\n\u001b[0;32m   1246\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1248\u001b[0m         docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39msimilarity_search(query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_kwargs)\n\u001b[0;32m   1249\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity_score_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1250\u001b[0m         docs_and_similarities \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1251\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39msimilarity_search_with_relevance_scores(\n\u001b[0;32m   1252\u001b[0m                 query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_kwargs\n\u001b[0;32m   1253\u001b[0m             )\n\u001b[0;32m   1254\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:530\u001b[0m, in \u001b[0;36mFAISS.similarity_search\u001b[1;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search\u001b[39m(\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    512\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    517\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m    518\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \n\u001b[0;32m    520\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;124;03m        List of Documents most similar to the query.\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 530\u001b[0m     docs_and_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_search_with_score(\n\u001b[0;32m    531\u001b[0m         query, k, \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfilter\u001b[39m, fetch_k\u001b[38;5;241m=\u001b[39mfetch_k, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    532\u001b[0m     )\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[1;32mc:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:403\u001b[0m, in \u001b[0;36mFAISS.similarity_search_with_score\u001b[1;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \n\u001b[0;32m    388\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m    L2 distance in float. Lower score represents more similarity.\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    402\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_query(query)\n\u001b[1;32m--> 403\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_search_with_score_by_vector(\n\u001b[0;32m    404\u001b[0m     embedding,\n\u001b[0;32m    405\u001b[0m     k,\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfilter\u001b[39m,\n\u001b[0;32m    407\u001b[0m     fetch_k\u001b[38;5;241m=\u001b[39mfetch_k,\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    409\u001b[0m )\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "File \u001b[1;32mc:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:304\u001b[0m, in \u001b[0;36mFAISS.similarity_search_with_score_by_vector\u001b[1;34m(self, embedding, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_L2:\n\u001b[0;32m    303\u001b[0m     faiss\u001b[38;5;241m.\u001b[39mnormalize_L2(vector)\n\u001b[1;32m--> 304\u001b[0m scores, indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39msearch(vector, k \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m fetch_k)\n\u001b[0;32m    305\u001b[0m docs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\faiss\\class_wrappers.py:329\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_search\u001b[1;34m(self, x, k, params, D, I)\u001b[0m\n\u001b[0;32m    327\u001b[0m n, d \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    328\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 329\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m d \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m k \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m D \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "doc = Document()\n",
    "question = '''문서 요약 관련 보고서를 만들어줘 \n",
    "    'title': '보고서의 제목',\n",
    "    'content':list['보고서의 목차별 제목'] 20글자 이내,\n",
    "    'summary': '보고서의 개요' 1000글자 이내\n",
    "key is title, content, summary.\n",
    "Respond using JSON only.'''\n",
    "# question = '국민건강보험법에서 직장가입자를 구분하는 기준에 대해서 보고서를 만들어줘 `title`: str(보고서의 제목), `content`: list [보고서의 목차별 제목] 20글자 이내로 Respond using JSON only.'\n",
    "# question = ' text`: str(`1. Google AIStudio를 검색한 후 아래 사이트로 접속하고 로그인 -> ‘Gemini API 키 가져오기’ 클릭`에 대한 상세한 정보) resonse in JSON format. only use korean in answer'\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm_model_json,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    condense_question_prompt=rag_prompt,\n",
    "    memory=memory,\n",
    ")\n",
    "res = conversation_chain({'question': question})\n",
    "# print(res['chat_history'][1].content)\n",
    "response = res['chat_history'][1].content.replace('\\n', '').lstrip().rstrip()\n",
    "response = json.loads(response)\n",
    "print(response)\n",
    "title = response['title']\n",
    "doc.add_heading(title, level = 0)\n",
    "\n",
    "doc.add_heading('목차', level = 1)\n",
    "doc.add_paragraph('1. 개요')\n",
    "doc.add_paragraph('2. 본문')\n",
    "for n, cont in enumerate(response['content'], 1):\n",
    "    doc.add_paragraph(f'\\t2-{n}. {cont}')\n",
    "    \n",
    "doc.add_heading('1. 개요', level = 1)\n",
    "doc.add_paragraph(response['summary'])\n",
    "\n",
    "doc.add_heading('2. 본문', level = 1)\n",
    "for n, cont in enumerate(response['content'], 1):\n",
    "    question = f'''{title}라는 보고서의 {cont} 부분 상세 내용 markdown 형식으로'''\n",
    "\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key='chat_history',\n",
    "        return_messages=True,\n",
    "    )\n",
    "\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm_model,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        condense_question_prompt=rag_prompt,\n",
    "        memory=memory,\n",
    "    )\n",
    "    \n",
    "    res = conversation_chain({'question': question})\n",
    "    print(res['chat_history'][1].content)\n",
    "    # response = res['chat_history'][1].content.replace('\\n', '').lstrip().rstrip()\n",
    "    # response = json.loads(response)\n",
    "    # print(response)\n",
    "    doc.add_heading(f'\\t2-{n}. {cont}', level = 2)\n",
    "    doc.add_paragraph(res['chat_history'][1].content)\n",
    "    \n",
    "    # if 'needed' in response:\n",
    "    #     doc.add_paragraph('조사가 필요한 내용')\n",
    "    #     for need in response['needed']:\n",
    "    #         doc.add_paragraph('\\t'+need)\n",
    "    \n",
    "doc.save('임베딩 테스트.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '국민건강보험법 하의 직장가입자 구분 기준',\n",
       " 'content': [{'title': '1. 적용 대상자',\n",
       "   'content': '모든 사업장 근로자와 사용자, 공무원 및 교직원은 직장가입자가 됩니다(법 제6조제2항). 5인 미만의 사업장의 근로자는 소득파악의 어려움으로 인해 직장가입자에 포함되지 않았으나, 이후 모든 사업장 근로자를 직장가입자로 확대하였습니다.'},\n",
       "  {'title': '2. 적용 범위',\n",
       "   'content': '직장가입자 범위는 상근근로자와 비상근근로자 모두를 포함하며, 1개월 동안 소정근로시간이 60시간 미만인 단시간근로자도 포함됩니다.'},\n",
       "  {'title': '3. 적용 제외 대상',\n",
       "   'content': '직장가입자에서 제외되는 대상은 일용근로자, 병역법상 현역병, 전환복무된 사람 및 군간부후보생, 매월 보수 또는 보수에 준하는 급료를 받지 않는 공무원입니다.'},\n",
       "  {'title': '4. 적용 범위',\n",
       "   'content': '직장가입자 범위는 상근근로자와 비상근근로자 모두를 포함하며, 1개월 동안 소정근로시간이 60시간 미만인 단시간근로자도 포함됩니다.'},\n",
       "  {'title': '5. 적용 제외 대상',\n",
       "   'content': '직장가입자에서 제외되는 대상은 일용근로자, 병역법상 현역병, 전환복무된 사람 및 군간부후보생, 매월 보수 또는 보수에 준하는 급료를 받지 않는 공무원입니다.'},\n",
       "  {'title': '6. 적용 범위',\n",
       "   'content': '직장가입자 범위는 상근근로자와 비상근근로자 모두를 포함하며, 1개월 동안 소정근로시간이 60시간 미만인 단시간근로자도 포함됩니다.'},\n",
       "  {'title': '7. 적용 제외 대상',\n",
       "   'content': '직장가입자에서 제외되는 대상은 일용근로자, 병역법상 현역병, 전환복무된 사람 및 군간부후보생, 매월 보수 또는 보수에 준하는 급료를 받지 않는 공무원입니다.'}],\n",
       " 'summary': '국민건강보험법은 직장가입자를 모든 사업장 근로자와 사용자, 공무원 및 교직원으로 규정하고 있습니다. 적용 범위에는 상근근로자, 비상근근로자, 1개월 동안 소정근로시간이 60시간 미만인 단시간근로자도 포함됩니다. 일용근로자, 병역법상 현역병, 전환복무된 사람 및 군간부후보생, 매월 보수 또는 보수에 준하는 급료를 받지 않는 공무원은 직장가입자에 해당하지 않습니다.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evee_res = response\n",
    "evee_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "FAISS.from_texts() missing 1 required positional argument: 'embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m         text_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m corrected_text \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     25\u001b[0m splits \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_text(text_sum)\n\u001b[1;32m---> 27\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_texts(splits)\n\u001b[0;32m     28\u001b[0m vectorstore\u001b[38;5;241m.\u001b[39msave_local(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: FAISS.from_texts() missing 1 required positional argument: 'embedding'"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from docx import Document\n",
    "import torch\n",
    "import json\n",
    "import faiss\n",
    "from langchain.vectorstores import FAISS\n",
    "import os\n",
    "\n",
    "# Define configuration for embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs={'device': 'cpu'},  # Change to 'cuda' if using GPU\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# Load and process text\n",
    "text_sum = ''\n",
    "files = ['C:/Users/SMHRD/Desktop/실전역량/MoAI/[경북TP]산업기술R&D연구기획사업 최종보고서_최종제본.pdf']\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "for file in files:\n",
    "    reader = PdfReader(file)\n",
    "    for page in reader.pages:  # 페이지 별로 텍스트 추출\n",
    "        text = page.extract_text()\n",
    "        corrected_text = text.encode('utf-8', errors='ignore').decode('utf-8')  # 인코딩 오류 무시 및 텍스트 누적\n",
    "        text_sum += corrected_text + '\\n'\n",
    "splits = text_splitter.split_text(text_sum)\n",
    "\n",
    "# Create FAISS index\n",
    "vectorstore = FAISS.from_texts(splits, embedding_model)\n",
    "vectorstore.save_local('test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 새로운 인덱스 생성\n",
    "vectorstore = FAISS.from_documents(documents, embedding_model)\n",
    "vectorstore.save_local('test')  # 인덱스를 로컬에 저장\n",
    "\n",
    "# 추후 로드할 때는 이전에 사용한 코드 그대로 사용\n",
    "vectorstore = FAISS.load_local(\n",
    "    'test',\n",
    "    embedding_model,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "   ---------------------------------------- 0.0/232.6 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/232.6 kB ? eta -:--:--\n",
      "   ----------------------------------- ---- 204.8/232.6 kB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 232.6/232.6 kB 3.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pyPDF2\n",
      "Successfully installed pyPDF2-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 임베딩 모델 설정\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 텍스트 추출 및 처리\n",
    "text_sum = ''\n",
    "files = ['[경북TP]산업기술R&D연구기획사업 최종보고서_최종제본.pdf']  # 문제 발생 가능성 있는 경로를 간소화\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "for file in files:\n",
    "    reader = PdfReader(file)\n",
    "    for page in reader.pages:\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            corrected_text = text.encode('utf-8', errors='ignore').decode('utf-8')\n",
    "            text_sum += corrected_text + '\\n'\n",
    "\n",
    "# 텍스트 분할 및 벡터 스토어 생성\n",
    "splits = text_splitter.split_text(text_sum)\n",
    "vectorstore = FAISS.from_texts(splits, embedding_model)\n",
    "\n",
    "# 벡터 스토어 저장\n",
    "vectorstore.save_local('test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install psycopg2\n",
    "import psycopg2\n",
    "from psycopg2 import pool\n",
    "\n",
    "db = psycopg2.connect(\n",
    "    user='postgres.vpcdvbdktvvzrvjfyyzm',\n",
    "    password='Odvv8E1iChKjwai4',\n",
    "    host='aws-0-ap-southeast-1.pooler.supabase.com',\n",
    "    port=6543,\n",
    "    dbname='postgres'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = psycopg2.connect(\n",
    "    user='moai',\n",
    "    password='smhrd1234',\n",
    "    host='project-db-campus.smhrd.com',\n",
    "    port=3310,\n",
    "    dbname='moai'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain_postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_postgres.vectorstores import PGVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs = {'device': device}, # 모델이 CPU에서 실행되도록 설정. GPU를 사용할 수 있는 환경이라면 'cuda'로 설정할 수도 있음\n",
    "    encode_kwargs = {'normalize_embeddings': True}, # 임베딩 정규화. 모든 벡터가 같은 범위의 값을 갖도록 함. 유사도 계산 시 일관성을 높여줌\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = PGVector(\n",
    "    embeddings=embedding_model,\n",
    "    collection_name='test_docs',\n",
    "    connection='postgresql+psycopg2://postgres.vpcdvbdktvvzrvjfyyzm:Odvv8E1iChKjwai4@aws-0-ap-southeast-1.pooler.supabase.com:6543/postgres',\n",
    "    use_jsonb=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs.delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid.uuid4().hex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pypdf import PdfReader\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_sum = ''\n",
    "files = ['./03.조건문.pdf',]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "for file in files:\n",
    "    reader = PdfReader(file)\n",
    "    for page in reader.pages: #페이지 별로 텍스트 추출\n",
    "        text = page.extract_text()\n",
    "        corrected_text = text.encode('utf-8', errors='ignore').decode('utf-8') #인코딩 오류 무시 및 텍스트 누적\n",
    "        text_sum += corrected_text +'\\n'\n",
    "splits = text_splitter.split_text(text_sum)\n",
    "\n",
    "vectorstore = FAISS.from_texts(splits)\n",
    "vectorstore.save_local('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_text(text_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs.add_texts(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://localhost:5100\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [28/Aug/2024 12:05:34] \"POST /summary HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [28/Aug/2024 12:07:24] \"POST /summary HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [28/Aug/2024 12:17:00] \"POST /summary HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [28/Aug/2024 12:19:26] \"POST /summary HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [28/Aug/2024 12:20:29] \"POST /summary HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from docx import Document\n",
    "from flask import *\n",
    "from flask_cors import CORS\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "app = Flask(__name__)\n",
    "# CORS(app, resources={r'*': {'origins': 'http://localhost:8000'}})\n",
    "CORS(app)\n",
    "host = 'localhost'\n",
    "port = 5100\n",
    "\n",
    "# Configuration\n",
    "ngrok = 'https://6c82-35-229-167-67.ngrok-free.app'  # Replace with your ngrok URL\n",
    "device = 'cpu'\n",
    "\n",
    "# Load language model\n",
    "llm_model = ChatOllama(\n",
    "    model='meta-llama-3.1',\n",
    "    base_url=ngrok\n",
    ")\n",
    "\n",
    "# Load embeddings model\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# Define prompt\n",
    "prompt_template = '''Use the following pieces of context to answer the question at the end.\n",
    "If you don't find the answer in context, don't try to make up an answer.\n",
    "If you find the answer in context, answer me only use korean.\n",
    "\n",
    "context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:'''\n",
    "rag_prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Function to create the DOCX file\n",
    "def create_docx(response, vectorstore):\n",
    "    doc = Document()\n",
    "    title = response.get('title', '제목 없음')\n",
    "    doc.add_heading(title, level=0)\n",
    "\n",
    "    doc.add_heading('목차', level=1)\n",
    "    doc.add_paragraph('1. 개요')\n",
    "    doc.add_paragraph('2. 본문')\n",
    "    for n, cont in enumerate(response.get('content', []), 1):\n",
    "        doc.add_paragraph(f'\\t2-{n}. {cont}')\n",
    "\n",
    "    doc.add_heading('1. 개요', level=1)\n",
    "    doc.add_paragraph(response.get('summary', ''))\n",
    "\n",
    "    doc.add_heading('2. 본문', level=1)\n",
    "    # for n, cont in enumerate(response.get('content', []), 1):\n",
    "    #     question = f'{title}라는 보고서의 {cont} 부분 상세 내용을 글로 풀어서 적어줘'    # {title}라는 보고서의 {cont} 부분 상세 내용 markdown 형식으로\n",
    "    #     memory = ConversationBufferMemory(\n",
    "    #         memory_key='chat_history',\n",
    "    #         return_messages=True\n",
    "    #     )\n",
    "    #     conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    #         llm=llm_model,\n",
    "    #         retriever=vectorstore.as_retriever(),\n",
    "    #         condense_question_prompt=rag_prompt,\n",
    "    #         memory=memory\n",
    "    #     )\n",
    "    #     res = conversation_chain({'question': question})\n",
    "    #     doc.add_heading(f'\\t2-{n}. {cont}', level=2)\n",
    "    #     doc.add_paragraph(res['chat_history'][1].content)\n",
    "    \n",
    "    # Format date to avoid issues in filenames\n",
    "    doc.save(f'./processed/{datetime.datetime.today().strftime(\"%g%m%d%H%M%S\")}.docx')\n",
    "    return f'/processed/{datetime.datetime.today().strftime(\"%g%m%d%H%M%S\")}.docx'\n",
    "\n",
    "def process_file(file_path):\n",
    "    # if not os.path.isfile(file_path):\n",
    "    #     print(f'파일이 존재하지 않습니다: {file_path}')\n",
    "    #     return\n",
    "    \n",
    "    text_sum = ''\n",
    "    # files = [file_path]\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    for file in file_path:\n",
    "        reader = PdfReader(file)\n",
    "        for page in reader.pages:  # 페이지 별로 텍스트 추출\n",
    "            text = page.extract_text()\n",
    "            corrected_text = text.encode('utf-8', errors='ignore').decode('utf-8')  # 인코딩 오류 무시 및 텍스트 누적\n",
    "            text_sum += corrected_text + '\\n'\n",
    "    splits = text_splitter.split_text(text_sum)\n",
    "\n",
    "    # Create FAISS index\n",
    "    vectorstore = FAISS.from_texts(splits, embedding_model)\n",
    "    \n",
    "    # Assuming text processing and JSON generation happens here\n",
    "    # For demonstration, we're creating a sample response\n",
    "    response = {\n",
    "        'title': '보고서 제목',\n",
    "        'content': ['목차 1', '목차 2'],\n",
    "        'summary': '보고서 개요'\n",
    "    }\n",
    "\n",
    "    return create_docx(response, vectorstore)\n",
    "\n",
    "@app.route('/summary', methods=['POST'])\n",
    "def summary():\n",
    "    \n",
    "    files = [request.files[i] for i in request.files]\n",
    "    processedFilePath = process_file(files)\n",
    "    return jsonify({\n",
    "        'result': 'ok',\n",
    "        'processedFilePath': processedFilePath\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host= host, port=port)\n",
    "    \n",
    "    # if len(sys.argv) != 2:\n",
    "    #     print(\"Usage: python ollama.py <file_path>\")\n",
    "    # else:\n",
    "    #     process_file(sys.argv[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'240828120342'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.today().strftime(\"%g%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-08-28 08/28/24'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
