{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import ollama\n",
    "\n",
    "res = ollama.chat(model='llama3-ko', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': '코사인의 미분 공식에 대해 설명해줘'\n",
    "    }\n",
    "])\n",
    "print(res['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from docx import Document\n",
    "import torch\n",
    "import json\n",
    "import faiss\n",
    "from langchain.vectorstores import FAISS\n",
    "# from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "ngrok = 'https://7505-34-125-71-39.ngrok-free.app'\n",
    "llm_model_json = ChatOllama(\n",
    "    model='meta-llama-3.1',\n",
    "    # num_predict=256,\n",
    "    format='json',\n",
    "    base_url=ngrok\n",
    ")\n",
    "llm_model = ChatOllama(\n",
    "    model='meta-llama-3.1',\n",
    "    # num_predict=256,\n",
    "    # format='json',\n",
    "    base_url=ngrok\n",
    ")\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs = {'device': device}, # 모델이 CPU에서 실행되도록 설정. GPU를 사용할 수 있는 환경이라면 'cuda'로 설정할 수도 있음\n",
    "    encode_kwargs = {'normalize_embeddings': True}, # 임베딩 정규화. 모든 벡터가 같은 범위의 값을 갖도록 함. 유사도 계산 시 일관성을 높여줌\n",
    ")\n",
    "\n",
    "\n",
    "prompt_template = '''Use the following pieces of context to answer the question at the end.\n",
    "If you don't find the answer in context, don't try to make up an answer.\n",
    "If you find the answer in context, answer me only use korean.\n",
    "\n",
    "context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:'''\n",
    "rag_prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# vectorstore = FAISS.load_local(\n",
    "#     'test',\n",
    "#     embedding_model,\n",
    "#     allow_dangerous_deserialization=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barista and ballerina's relationship\n"
     ]
    }
   ],
   "source": [
    "ans = llm_model.invoke('translate to english this sentence, \"바리스타와 발리스타의 관계\". answer me only translated sentence.')\n",
    "print(ans.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바리스타와 발레리나의 관계\n"
     ]
    }
   ],
   "source": [
    "ans = llm_model.invoke('translate to korean this sentence, \"Barista and ballerina\\'s relationship\". answer me only translated sentence.')\n",
    "print(ans.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 22\u001b[0m\n\u001b[0;32m     11\u001b[0m memory \u001b[38;5;241m=\u001b[39m ConversationBufferMemory(\n\u001b[0;32m     12\u001b[0m     memory_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     13\u001b[0m     return_messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     16\u001b[0m conversation_chain \u001b[38;5;241m=\u001b[39m ConversationalRetrievalChain\u001b[38;5;241m.\u001b[39mfrom_llm(\n\u001b[0;32m     17\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm_model_json,\n\u001b[0;32m     18\u001b[0m     retriever\u001b[38;5;241m=\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39mas_retriever(),\n\u001b[0;32m     19\u001b[0m     condense_question_prompt\u001b[38;5;241m=\u001b[39mrag_prompt,\n\u001b[0;32m     20\u001b[0m     memory\u001b[38;5;241m=\u001b[39mmemory,\n\u001b[0;32m     21\u001b[0m )\n\u001b[1;32m---> 22\u001b[0m res \u001b[38;5;241m=\u001b[39m conversation_chain({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m: question})\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# print(res['chat_history'][1].content)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m response \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mlstrip()\u001b[38;5;241m.\u001b[39mrstrip()\n",
      "File \u001b[1;32mc:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     emit_warning()\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:383\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    376\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    381\u001b[0m }\n\u001b[1;32m--> 383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m    384\u001b[0m     inputs,\n\u001b[0;32m    385\u001b[0m     cast(RunnableConfig, {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}),\n\u001b[0;32m    386\u001b[0m     return_only_outputs\u001b[38;5;241m=\u001b[39mreturn_only_outputs,\n\u001b[0;32m    387\u001b[0m     include_run_info\u001b[38;5;241m=\u001b[39minclude_run_info,\n\u001b[0;32m    388\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:166\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    161\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    162\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\conversational_retrieval\\base.py:160\u001b[0m, in \u001b[0;36mBaseConversationalRetrievalChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    156\u001b[0m accepts_run_manager \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_docs)\u001b[38;5;241m.\u001b[39mparameters\n\u001b[0;32m    158\u001b[0m )\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accepts_run_manager:\n\u001b[1;32m--> 160\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_docs(new_question, inputs, run_manager\u001b[38;5;241m=\u001b[39m_run_manager)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_docs(new_question, inputs)  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\conversational_retrieval\\base.py:397\u001b[0m, in \u001b[0;36mConversationalRetrievalChain._get_docs\u001b[1;34m(self, question, inputs, run_manager)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_docs\u001b[39m(\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    391\u001b[0m     question: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m     run_manager: CallbackManagerForChainRun,\n\u001b[0;32m    395\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m    396\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get docs.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 397\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretriever\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m    398\u001b[0m         question, config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_manager\u001b[38;5;241m.\u001b[39mget_child()}\n\u001b[0;32m    399\u001b[0m     )\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reduce_tokens_below_limit(docs)\n",
      "File \u001b[1;32mc:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\retrievers.py:221\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    220\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_error(e)\n\u001b[1;32m--> 221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_end(\n\u001b[0;32m    224\u001b[0m         result,\n\u001b[0;32m    225\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\retrievers.py:214\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[1;32m--> 214\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;28minput\u001b[39m, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs\n\u001b[0;32m    216\u001b[0m     )\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:1248\u001b[0m, in \u001b[0;36mVectorStoreRetriever._get_relevant_documents\u001b[1;34m(self, query, run_manager)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_relevant_documents\u001b[39m(\n\u001b[0;32m   1245\u001b[0m     \u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m, run_manager: CallbackManagerForRetrieverRun\n\u001b[0;32m   1246\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1248\u001b[0m         docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39msimilarity_search(query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_kwargs)\n\u001b[0;32m   1249\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity_score_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1250\u001b[0m         docs_and_similarities \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1251\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39msimilarity_search_with_relevance_scores(\n\u001b[0;32m   1252\u001b[0m                 query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_kwargs\n\u001b[0;32m   1253\u001b[0m             )\n\u001b[0;32m   1254\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:530\u001b[0m, in \u001b[0;36mFAISS.similarity_search\u001b[1;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search\u001b[39m(\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    512\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    517\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m    518\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \n\u001b[0;32m    520\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;124;03m        List of Documents most similar to the query.\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 530\u001b[0m     docs_and_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_search_with_score(\n\u001b[0;32m    531\u001b[0m         query, k, \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfilter\u001b[39m, fetch_k\u001b[38;5;241m=\u001b[39mfetch_k, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    532\u001b[0m     )\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[1;32mc:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:403\u001b[0m, in \u001b[0;36mFAISS.similarity_search_with_score\u001b[1;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \n\u001b[0;32m    388\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m    L2 distance in float. Lower score represents more similarity.\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    402\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_query(query)\n\u001b[1;32m--> 403\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_search_with_score_by_vector(\n\u001b[0;32m    404\u001b[0m     embedding,\n\u001b[0;32m    405\u001b[0m     k,\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfilter\u001b[39m,\n\u001b[0;32m    407\u001b[0m     fetch_k\u001b[38;5;241m=\u001b[39mfetch_k,\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    409\u001b[0m )\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "File \u001b[1;32mc:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:304\u001b[0m, in \u001b[0;36mFAISS.similarity_search_with_score_by_vector\u001b[1;34m(self, embedding, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_L2:\n\u001b[0;32m    303\u001b[0m     faiss\u001b[38;5;241m.\u001b[39mnormalize_L2(vector)\n\u001b[1;32m--> 304\u001b[0m scores, indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39msearch(vector, k \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m fetch_k)\n\u001b[0;32m    305\u001b[0m docs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\faiss\\class_wrappers.py:329\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_search\u001b[1;34m(self, x, k, params, D, I)\u001b[0m\n\u001b[0;32m    327\u001b[0m n, d \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    328\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 329\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m d \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m k \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m D \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "doc = Document()\n",
    "question = '''문서 요약 관련 보고서를 만들어줘 \n",
    "    'title': '보고서의 제목',\n",
    "    'content':list['보고서의 목차별 제목'] 20글자 이내,\n",
    "    'summary': '보고서의 개요' 1000글자 이내\n",
    "key is title, content, summary.\n",
    "Respond using JSON only.'''\n",
    "# question = '국민건강보험법에서 직장가입자를 구분하는 기준에 대해서 보고서를 만들어줘 `title`: str(보고서의 제목), `content`: list [보고서의 목차별 제목] 20글자 이내로 Respond using JSON only.'\n",
    "# question = ' text`: str(`1. Google AIStudio를 검색한 후 아래 사이트로 접속하고 로그인 -> ‘Gemini API 키 가져오기’ 클릭`에 대한 상세한 정보) resonse in JSON format. only use korean in answer'\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm_model_json,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    condense_question_prompt=rag_prompt,\n",
    "    memory=memory,\n",
    ")\n",
    "res = conversation_chain({'question': question})\n",
    "# print(res['chat_history'][1].content)\n",
    "response = res['chat_history'][1].content.replace('\\n', '').lstrip().rstrip()\n",
    "response = json.loads(response)\n",
    "print(response)\n",
    "title = response['title']\n",
    "doc.add_heading(title, level = 0)\n",
    "\n",
    "doc.add_heading('목차', level = 1)\n",
    "doc.add_paragraph('1. 개요')\n",
    "doc.add_paragraph('2. 본문')\n",
    "for n, cont in enumerate(response['content'], 1):\n",
    "    doc.add_paragraph(f'\\t2-{n}. {cont}')\n",
    "    \n",
    "doc.add_heading('1. 개요', level = 1)\n",
    "doc.add_paragraph(response['summary'])\n",
    "\n",
    "doc.add_heading('2. 본문', level = 1)\n",
    "for n, cont in enumerate(response['content'], 1):\n",
    "    question = f'''{title}라는 보고서의 {cont} 부분 상세 내용 markdown 형식으로'''\n",
    "\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key='chat_history',\n",
    "        return_messages=True,\n",
    "    )\n",
    "\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm_model,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        condense_question_prompt=rag_prompt,\n",
    "        memory=memory,\n",
    "    )\n",
    "    \n",
    "    res = conversation_chain({'question': question})\n",
    "    print(res['chat_history'][1].content)\n",
    "    # response = res['chat_history'][1].content.replace('\\n', '').lstrip().rstrip()\n",
    "    # response = json.loads(response)\n",
    "    # print(response)\n",
    "    doc.add_heading(f'\\t2-{n}. {cont}', level = 2)\n",
    "    doc.add_paragraph(res['chat_history'][1].content)\n",
    "    \n",
    "    # if 'needed' in response:\n",
    "    #     doc.add_paragraph('조사가 필요한 내용')\n",
    "    #     for need in response['needed']:\n",
    "    #         doc.add_paragraph('\\t'+need)\n",
    "    \n",
    "doc.save('임베딩 테스트.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '국민건강보험법 하의 직장가입자 구분 기준',\n",
       " 'content': [{'title': '1. 적용 대상자',\n",
       "   'content': '모든 사업장 근로자와 사용자, 공무원 및 교직원은 직장가입자가 됩니다(법 제6조제2항). 5인 미만의 사업장의 근로자는 소득파악의 어려움으로 인해 직장가입자에 포함되지 않았으나, 이후 모든 사업장 근로자를 직장가입자로 확대하였습니다.'},\n",
       "  {'title': '2. 적용 범위',\n",
       "   'content': '직장가입자 범위는 상근근로자와 비상근근로자 모두를 포함하며, 1개월 동안 소정근로시간이 60시간 미만인 단시간근로자도 포함됩니다.'},\n",
       "  {'title': '3. 적용 제외 대상',\n",
       "   'content': '직장가입자에서 제외되는 대상은 일용근로자, 병역법상 현역병, 전환복무된 사람 및 군간부후보생, 매월 보수 또는 보수에 준하는 급료를 받지 않는 공무원입니다.'},\n",
       "  {'title': '4. 적용 범위',\n",
       "   'content': '직장가입자 범위는 상근근로자와 비상근근로자 모두를 포함하며, 1개월 동안 소정근로시간이 60시간 미만인 단시간근로자도 포함됩니다.'},\n",
       "  {'title': '5. 적용 제외 대상',\n",
       "   'content': '직장가입자에서 제외되는 대상은 일용근로자, 병역법상 현역병, 전환복무된 사람 및 군간부후보생, 매월 보수 또는 보수에 준하는 급료를 받지 않는 공무원입니다.'},\n",
       "  {'title': '6. 적용 범위',\n",
       "   'content': '직장가입자 범위는 상근근로자와 비상근근로자 모두를 포함하며, 1개월 동안 소정근로시간이 60시간 미만인 단시간근로자도 포함됩니다.'},\n",
       "  {'title': '7. 적용 제외 대상',\n",
       "   'content': '직장가입자에서 제외되는 대상은 일용근로자, 병역법상 현역병, 전환복무된 사람 및 군간부후보생, 매월 보수 또는 보수에 준하는 급료를 받지 않는 공무원입니다.'}],\n",
       " 'summary': '국민건강보험법은 직장가입자를 모든 사업장 근로자와 사용자, 공무원 및 교직원으로 규정하고 있습니다. 적용 범위에는 상근근로자, 비상근근로자, 1개월 동안 소정근로시간이 60시간 미만인 단시간근로자도 포함됩니다. 일용근로자, 병역법상 현역병, 전환복무된 사람 및 군간부후보생, 매월 보수 또는 보수에 준하는 급료를 받지 않는 공무원은 직장가입자에 해당하지 않습니다.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evee_res = response\n",
    "evee_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from docx import Document\n",
    "import torch\n",
    "import json\n",
    "import faiss\n",
    "from langchain.vectorstores import FAISS\n",
    "import os\n",
    "\n",
    "# Define configuration for embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs={'device': 'cpu'},  # Change to 'cuda' if using GPU\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# Load and process text\n",
    "text_sum = ''\n",
    "files = ['03.조건문.pdf']\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "for file in files:\n",
    "    reader = PdfReader(file)\n",
    "    for page in reader.pages:  # 페이지 별로 텍스트 추출\n",
    "        text = page.extract_text()\n",
    "        corrected_text = text.encode('utf-8', errors='ignore').decode('utf-8')  # 인코딩 오류 무시 및 텍스트 누적\n",
    "        text_sum += corrected_text + '\\n'\n",
    "splits = text_splitter.split_text(text_sum)\n",
    "\n",
    "# Create FAISS index\n",
    "vectorstore = FAISS.from_texts(splits, embedding_model)\n",
    "vectorstore.save_local('test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 새로운 인덱스 생성\n",
    "vectorstore = FAISS.from_documents(documents, embedding_model)\n",
    "vectorstore.save_local('test')  # 인덱스를 로컬에 저장\n",
    "\n",
    "# 추후 로드할 때는 이전에 사용한 코드 그대로 사용\n",
    "vectorstore = FAISS.load_local(\n",
    "    'test',\n",
    "    embedding_model,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsFaiss = vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "   ---------------------------------------- 0.0/232.6 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/232.6 kB ? eta -:--:--\n",
      "   ----------------------------------- ---- 204.8/232.6 kB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 232.6/232.6 kB 3.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pyPDF2\n",
      "Successfully installed pyPDF2-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PyPDF2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FAISS\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPyPDF2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PdfReader\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 임베딩 모델 설정\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'PyPDF2'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 임베딩 모델 설정\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 텍스트 추출 및 처리\n",
    "text_sum = ''\n",
    "files = ['03.조건문.pdf']  # 문제 발생 가능성 있는 경로를 간소화\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "for file in files:\n",
    "    reader = PdfReader(file)\n",
    "    for page in reader.pages:\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            corrected_text = text.encode('utf-8', errors='ignore').decode('utf-8')\n",
    "            text_sum += corrected_text + '\\n'\n",
    "\n",
    "# 텍스트 분할 및 벡터 스토어 생성\n",
    "splits = text_splitter.split_text(text_sum)\n",
    "vectorstore = FAISS.from_texts(splits, embedding_model)\n",
    "\n",
    "# 벡터 스토어 저장\n",
    "vectorstore.save_local('test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install psycopg2\n",
    "import psycopg2\n",
    "from psycopg2 import pool\n",
    "\n",
    "db = psycopg2.connect(\n",
    "    user='postgres.vpcdvbdktvvzrvjfyyzm',\n",
    "    password='Odvv8E1iChKjwai4',\n",
    "    host='aws-0-ap-southeast-1.pooler.supabase.com',\n",
    "    port=6543,\n",
    "    dbname='postgres'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (842848883.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[10], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    user: 'postgres.vpcdvbdktvvzrvjfyyzm',\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "db = psycopg2.connect(\n",
    "    user: 'postgres.vpcdvbdktvvzrvjfyyzm',\n",
    "    host: 'aws-0-ap-southeast-1.pooler.supabase.com',\n",
    "    database: 'postgres',\n",
    "    password: 'Odvv8E1iChKjwai4',\n",
    "    port: 6543,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain_postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_postgres.vectorstores import PGVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "device='cpu'\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs = {'device': device}, # 모델이 CPU에서 실행되도록 설정. GPU를 사용할 수 있는 환경이라면 'cuda'로 설정할 수도 있음\n",
    "    encode_kwargs = {'normalize_embeddings': True}, # 임베딩 정규화. 모든 벡터가 같은 범위의 값을 갖도록 함. 유사도 계산 시 일관성을 높여줌\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = PGVector(\n",
    "    embeddings=embedding_model,\n",
    "    collection_name='user_id',\n",
    "    connection='postgresql+psycopg2://postgres.vpcdvbdktvvzrvjfyyzm:Odvv8E1iChKjwai4@aws-0-ap-southeast-1.pooler.supabase.com:6543/postgres',\n",
    "    use_jsonb=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {'key': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic.get('val') == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_sum = ''\n",
    "files = ['./03.조건문.pdf',]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "for file in files:\n",
    "    reader = PdfReader(file)\n",
    "    for page in reader.pages: #페이지 별로 텍스트 추출\n",
    "        text = page.extract_text()\n",
    "        corrected_text = text.encode('utf-8', errors='ignore').decode('utf-8') #인코딩 오류 무시 및 텍스트 누적\n",
    "        text_sum += corrected_text +'\\n'\n",
    "splits = text_splitter.split_text(text_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = vs.add_texts(splits, ids=[1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['PGVector', 'HuggingFaceEmbeddings'], vectorstore=<langchain_postgres.vectorstores.PGVector object at 0x00000246BA1F43D0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_text(text_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이 도 연 연구원\\n\\n학습목표\\n조건문의의미와종류를알아본다 .\\n다중 if문을활용해문제를해결할수있다.\\n다중 if문과단순 if문여러개의차이점을알아본다 .\\n\\n조건문이란 ?(conditional statement)\\n2000원현재잔액 : 2500원\\n3000원\\n\\n조건문이란 ?(conditional statement)\\n로그인\\n\\n조건문이란 ?(conditional statement)\\n주어진조건을비교판단하여\\n그조건에만족할경우지정된명령을실행하고 , \\n만족하지않을경우다음단계의명령을수행\\n조건만족 지정된  명령\\n맞지 않음다음 명령\\n조건문의종류\\n단순 if문 if-else문\\nswitch문다중 if문\\n(if-else if)\\n단순 if문\\n흐름도\\n시작\\n끝조건문\\n실행문장truefalse\\n문법\\n반드시  { } (중괄호 ) 작성\\n실행문장이  한 줄일 경우 \\n{ } (중괄호 ) 생략 가능조건식\\n조건문예제 -실습\\nint타입의 변수 age를선언하고 키보드로 값을 입력 받으세요 .\\n만약 age가20보다 크거나 같다면 “성인입니다 .”라고 출력하는\\n프로그램을 만들어보세요 .1. 단순 if문예제\\n\\n조건문예제 -실습\\nint타입의 변수 num 을선언하고 키보드로 값을 입력 받으세요 .\\n만약 num 이3의배수이면서 5의배수라면 “3과5의배수입니다 “라고\\n출력하는 프로그램을 만들어보세요 .2. 단순 if문예제\\n\\nif-else문\\n흐름도\\n시작\\n끝조건문\\n실행문장 1true false\\n실행문장 2문법\\ntrue 혹은 false 가 \\n나오게  작성\\n거짓일  경우 실행하는  구간\\n실행문장이  한 줄일 경우 \\n{ } (중괄호 ) 생략 가능\\n조건문예제 -실습\\nint타입의 변수 grade 를선언하고 키보드로 값을 입력 받으세요 .\\n만약 grade 가60점이상이면 “합격입니다 .”\\n그렇지 않다면 “불합격입니다 .”를출력하는 프로그램을 작성해보세요 .1.  if -else문예제',\n",
       " '조건문예제 -실습\\n다음은 에버랜드 입장료 계산 프로그램입니다 .\\n기본료는 5000 원이며 인원수에 따라 지불해야하는 프로그램을 만들어보세요 .\\n단, 20세미만인 경우 50% 할인이 적용됩니다 .2.  if -else문예제\\n\\n조건문예제 -실습\\n다음은 마트 계산대 프로그램입니다 .\\n10000 원짜리 선물세트를 구입했을 때지불해야하는 금액을 계산해 보세요 .\\n단, 11개이상 구매 시에는 전체금액에서 10% 할인이적용됩니다 .3.  if -else문예제\\n\\n흐름도다중 if문\\n문법\\n시작\\n끝조건문 1true\\nfalse\\n조건문 2\\n조건문 3실행문장 1\\n실행문장 2\\n실행문장 3true\\nfalse\\nfalsetrue\\n조건식 1의 값이 false 일 때\\nelse if 구문을  만나 조건식 2의 값 검사\\n조건문예제 -실습\\nint타입의 변수 totalScore 를선언하고 키보드로 값을 입력 받으세요 .\\ntotalScore 가90점이상이면 “A학점입니다 .”,\\n80점이상 90점미만일경우 “B학점입니다 .”,\\n70점이상 80점미만일경우 “C학점입니다 .”\\n70점미만일경우 “D학점입니다 .”를출력하는 프로그램을 만드세요 .1.  다중 if문예제\\n\\nswitch -case문\\n흐름도\\n시작\\n끝식==값1true\\nfalse\\n식==값2\\n식==값실행문장 1\\n실행문장 2\\n실행문장 3true\\nfalse\\nfalsetrue\\n문법\\n딱떨어지는  값이 \\n나오게  설정\\n(boolean  안됨)\\nelse 와 같은 역할\\n각각의 case마다꼭break넣기!\\n조건문예제 -실습\\n1.  switch 문예제\\nint타입의 변수 totalScore 를선언하고 키보드로 값을 입력 받으세요 .\\ntotalScore 가90점이상이면 “A학점입니다 .”,\\n80점이상 90점미만일경우 “B학점입니다 .”,\\n70점이상 80점미만일경우 “C학점입니다 .”\\n70점미만일경우 “D학점입니다 .”를출력하는 프로그램을 만드세요 .',\n",
       " '조건문예제 -실습\\n2.  switch 문예제\\n월을 입력 받아 봄, 여름, 가을, 겨울 중맞는 계절을 알려주는 프로그램을 만드세요 .\\n12,1,2월-> 겨울 3,4,5월-> 봄\\n6,7,8월-> 여름 9,10,11월-> 가을\\n\\n조건문최종예제–자판기프로그램\\n조건문복합예제\\n자판기 프로그램을 만들어봅시다 .\\n금액을 입력하고 메뉴를 고른 뒤잔돈을 출력해 보세요 !\\n\\nswitch -case문\\n조건문복합예제\\n입력한 금액이 선택한 메뉴의 가격보다 부족하면\\n“돈이 부족합니다 !” 문장을 출력하세요 !\\n\\nswitch -case문\\n조건문복합예제\\n잔돈을 줄때천원을 몇개줘야 하는지 계산해 보세요 !\\n\\nswitch -case문\\n조건문복합예제\\n동일한 방식으로 오백원 , 백원을 몇개줘야하는 지계산해보세요 !\\n\\n반복문\\n다음시간에  배울 내용']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs.add_texts(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://localhost:5100\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [28/Aug/2024 12:05:34] \"POST /summary HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [28/Aug/2024 12:07:24] \"POST /summary HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [28/Aug/2024 12:17:00] \"POST /summary HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [28/Aug/2024 12:19:26] \"POST /summary HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [28/Aug/2024 12:20:29] \"POST /summary HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from docx import Document\n",
    "from flask import *\n",
    "from flask_cors import CORS\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "app = Flask(__name__)\n",
    "# CORS(app, resources={r'*': {'origins': 'http://localhost:8000'}})\n",
    "CORS(app)\n",
    "host = 'localhost'\n",
    "port = 5100\n",
    "\n",
    "# Configuration\n",
    "ngrok = 'https://6c82-35-229-167-67.ngrok-free.app'  # Replace with your ngrok URL\n",
    "device = 'cpu'\n",
    "\n",
    "# Load language model\n",
    "llm_model = ChatOllama(\n",
    "    model='meta-llama-3.1',\n",
    "    base_url=ngrok\n",
    ")\n",
    "\n",
    "# Load embeddings model\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# Define prompt\n",
    "prompt_template = '''Use the following pieces of context to answer the question at the end.\n",
    "If you don't find the answer in context, don't try to make up an answer.\n",
    "If you find the answer in context, answer me only use korean.\n",
    "\n",
    "context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:'''\n",
    "rag_prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Function to create the DOCX file\n",
    "def create_docx(response, vectorstore):\n",
    "    doc = Document()\n",
    "    title = response.get('title', '제목 없음')\n",
    "    doc.add_heading(title, level=0)\n",
    "\n",
    "    doc.add_heading('목차', level=1)\n",
    "    doc.add_paragraph('1. 개요')\n",
    "    doc.add_paragraph('2. 본문')\n",
    "    for n, cont in enumerate(response.get('content', []), 1):\n",
    "        doc.add_paragraph(f'\\t2-{n}. {cont}')\n",
    "\n",
    "    doc.add_heading('1. 개요', level=1)\n",
    "    doc.add_paragraph(response.get('summary', ''))\n",
    "\n",
    "    doc.add_heading('2. 본문', level=1)\n",
    "    # for n, cont in enumerate(response.get('content', []), 1):\n",
    "    #     question = f'{title}라는 보고서의 {cont} 부분 상세 내용을 글로 풀어서 적어줘'    # {title}라는 보고서의 {cont} 부분 상세 내용 markdown 형식으로\n",
    "    #     memory = ConversationBufferMemory(\n",
    "    #         memory_key='chat_history',\n",
    "    #         return_messages=True\n",
    "    #     )\n",
    "    #     conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    #         llm=llm_model,\n",
    "    #         retriever=vectorstore.as_retriever(),\n",
    "    #         condense_question_prompt=rag_prompt,\n",
    "    #         memory=memory\n",
    "    #     )\n",
    "    #     res = conversation_chain({'question': question})\n",
    "    #     doc.add_heading(f'\\t2-{n}. {cont}', level=2)\n",
    "    #     doc.add_paragraph(res['chat_history'][1].content)\n",
    "    \n",
    "    # Format date to avoid issues in filenames\n",
    "    doc.save(f'./processed/{datetime.datetime.today().strftime(\"%g%m%d%H%M%S\")}.docx')\n",
    "    return f'/processed/{datetime.datetime.today().strftime(\"%g%m%d%H%M%S\")}.docx'\n",
    "\n",
    "def process_file(file_path):\n",
    "    # if not os.path.isfile(file_path):\n",
    "    #     print(f'파일이 존재하지 않습니다: {file_path}')\n",
    "    #     return\n",
    "    \n",
    "    text_sum = ''\n",
    "    # files = [file_path]\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    for file in file_path:\n",
    "        reader = PdfReader(file)\n",
    "        for page in reader.pages:  # 페이지 별로 텍스트 추출\n",
    "            text = page.extract_text()\n",
    "            corrected_text = text.encode('utf-8', errors='ignore').decode('utf-8')  # 인코딩 오류 무시 및 텍스트 누적\n",
    "            text_sum += corrected_text + '\\n'\n",
    "    splits = text_splitter.split_text(text_sum)\n",
    "\n",
    "    # Create FAISS index\n",
    "    vectorstore = FAISS.from_texts(splits, embedding_model)\n",
    "    \n",
    "    # Assuming text processing and JSON generation happens here\n",
    "    # For demonstration, we're creating a sample response\n",
    "    response = {\n",
    "        'title': '보고서 제목',\n",
    "        'content': ['목차 1', '목차 2'],\n",
    "        'summary': '보고서 개요'\n",
    "    }\n",
    "\n",
    "    return create_docx(response, vectorstore)\n",
    "\n",
    "@app.route('/summary', methods=['POST'])\n",
    "def summary():\n",
    "    \n",
    "    files = [request.files[i] for i in request.files]\n",
    "    processedFilePath = process_file(files)\n",
    "    return jsonify({\n",
    "        'result': 'ok',\n",
    "        'processedFilePath': processedFilePath\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host= host, port=port)\n",
    "    \n",
    "    # if len(sys.argv) != 2:\n",
    "    #     print(\"Usage: python ollama.py <file_path>\")\n",
    "    # else:\n",
    "    #     process_file(sys.argv[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'240828120342'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.today().strftime(\"%g%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-08-28 08/28/24'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
