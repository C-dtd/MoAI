{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import ollama\n",
    "\n",
    "res = ollama.chat(model='llama3-ko', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': '코사인의 미분 공식에 대해 설명해줘'\n",
    "    }\n",
    "])\n",
    "print(res['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask_socketio import SocketIO, emit, join_room, leave_room\n",
    "from flask import *\n",
    "from flask_login import *\n",
    "from flask_session import Session\n",
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# from datetime import timedelta\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "import torch\n",
    "\n",
    "llm_model = ChatOllama(\n",
    "    model='meta-llama-3.1',\n",
    "    num_predict=256,\n",
    "    format='json'\n",
    ")\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs = {'device': device}, # 모델이 CPU에서 실행되도록 설정. GPU를 사용할 수 있는 환경이라면 'cuda'로 설정할 수도 있음\n",
    "    encode_kwargs = {'normalize_embeddings': True}, # 임베딩 정규화. 모든 벡터가 같은 범위의 값을 갖도록 함. 유사도 계산 시 일관성을 높여줌\n",
    ")\n",
    "\n",
    "\n",
    "prompt_template = '''Use the following pieces of context to answer the question at the end.\n",
    "If you don't find the answer in context, just say that '내용을 확인할 수 없습니다.', don't try to make up an answer.\n",
    "If you find the answer in context, answer me only use korean.\n",
    "\n",
    "context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:'''\n",
    "rag_prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "vectorstore = FAISS.load_local(\n",
    "    'test',\n",
    "    embedding_model,\n",
    "    allow_dangerous_deserialization=True,\n",
    ")\n",
    "# params = request.get_json()\n",
    "# question = params['question']\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    "    return_messages=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \"content\": [\"1. Google AIStudio를검색한후아래사이트로접속및로그인-> ‘Gemini API 키가져오기’ 클릭\", \"2. 로그인후‘Get API Key’ 클릭\", \"3. ‘API 키만들기’ 클릭\", \"4. ‘새프로젝트에서API 키만들기’ 버튼클릭\", \"5. API Key 생성된정보확인\", \"6. 해당페이지에서API 키는재확인및복사가능\"] } \t                    \t\t                    \t\t                    \t\t                    \t\t                    \t\t                    \t\t\t\t\t                    \t\t\t\t\t                    \t\t\t\t\t                    \t\t\t\t\t                    \t\t\t\t\t                    \t\t\t\t\t                    \t\t\t\t\t                    \t\t\t\t\t                    \t\t\t\t\t                    \n"
     ]
    }
   ],
   "source": [
    "question = '`content`: list [보고서의 목차] resonse in JSON format.'\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm_model,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    condense_question_prompt=rag_prompt,\n",
    "    memory=memory,\n",
    ")\n",
    "res = conversation_chain({'question': question})\n",
    "# print(res['chat_history'][1].content)\n",
    "\n",
    "print(\n",
    "    res['chat_history'][1].content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': ['1. Google AIStudio를검색한후아래사이트로접속및로그인-> ‘Gemini API 키가져오기’ 클릭',\n",
       "  '2. 로그인후‘Get API Key’ 클릭',\n",
       "  '3. ‘API 키만들기’ 클릭',\n",
       "  '4. ‘새프로젝트에서API 키만들기’ 버튼클릭',\n",
       "  '5. API Key 생성된정보확인',\n",
       "  '6. 해당페이지에서API 키는재확인및복사가능']}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = { \"content\": [\"1. Google AIStudio를검색한후아래사이트로접속및로그인-> ‘Gemini API 키가져오기’ 클릭\", \"2. 로그인후‘Get API Key’ 클릭\", \"3. ‘API 키만들기’ 클릭\", \"4. ‘새프로젝트에서API 키만들기’ 버튼클릭\", \"5. API Key 생성된정보확인\", \"6. 해당페이지에서API 키는재확인및복사가능\"] }\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install psycopg2\n",
    "import psycopg2\n",
    "from psycopg2 import pool\n",
    "\n",
    "db = psycopg2.connect(\n",
    "    user='postgres.vpcdvbdktvvzrvjfyyzm',\n",
    "    password='Odvv8E1iChKjwai4',\n",
    "    host='aws-0-ap-southeast-1.pooler.supabase.com',\n",
    "    port=6543,\n",
    "    dbname='postgres'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
