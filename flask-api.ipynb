{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://localhost:5100\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [28/Aug/2024 14:42:57] \"GET / HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [28/Aug/2024 14:42:57] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "[2024-08-28 14:45:47,871] ERROR in app: Exception on /summary [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 2529, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 1825, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\flask_cors\\extension.py\", line 178, in wrapped_function\n",
      "    return cors_after_request(app.make_response(f(*args, **kwargs)))\n",
      "                                                ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 1823, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 1799, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SMHRD\\AppData\\Local\\Temp\\ipykernel_12480\\2618771965.py\", line 123, in summary\n",
      "    processedFilePath = process_file(files)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SMHRD\\AppData\\Local\\Temp\\ipykernel_12480\\2618771965.py\", line 117, in process_file\n",
      "    return create_docx(response, vectorstore)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SMHRD\\AppData\\Local\\Temp\\ipykernel_12480\\2618771965.py\", line 81, in create_docx\n",
      "    res = conversation_chain({'question': question})\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\conversational_retrieval\\base.py\", line 171, in _call\n",
      "    answer = self.combine_docs_chain.run(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 605, in run\n",
      "    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\combine_documents\\base.py\", line 138, in _call\n",
      "    output, extra_return_dict = self.combine_docs(\n",
      "                                ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\combine_documents\\stuff.py\", line 249, in combine_docs\n",
      "    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 318, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 128, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 140, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 714, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 571, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 561, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 793, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 286, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 217, in _chat_stream_with_aggregation\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 189, in _create_chat_stream\n",
      "    yield from self._create_stream(\n",
      "               ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 265, in _create_stream\n",
      "    raise OllamaEndpointNotFoundError(\n",
      "langchain_community.llms.ollama.OllamaEndpointNotFoundError: Ollama call failed with status code 404. Maybe your model is not found and you should pull the model with `ollama pull meta-llama-3.1`.\n",
      "127.0.0.1 - - [28/Aug/2024 14:45:47] \"POST /summary HTTP/1.1\" 500 -\n",
      "[2024-08-28 14:50:30,913] ERROR in app: Exception on /summary [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 2529, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 1825, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\flask_cors\\extension.py\", line 178, in wrapped_function\n",
      "    return cors_after_request(app.make_response(f(*args, **kwargs)))\n",
      "                                                ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 1823, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 1799, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SMHRD\\AppData\\Local\\Temp\\ipykernel_12480\\2618771965.py\", line 123, in summary\n",
      "    processedFilePath = process_file(files)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SMHRD\\AppData\\Local\\Temp\\ipykernel_12480\\2618771965.py\", line 117, in process_file\n",
      "    return create_docx(response, vectorstore)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SMHRD\\AppData\\Local\\Temp\\ipykernel_12480\\2618771965.py\", line 81, in create_docx\n",
      "    res = conversation_chain({'question': question})\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\conversational_retrieval\\base.py\", line 171, in _call\n",
      "    answer = self.combine_docs_chain.run(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 605, in run\n",
      "    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\combine_documents\\base.py\", line 138, in _call\n",
      "    output, extra_return_dict = self.combine_docs(\n",
      "                                ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\combine_documents\\stuff.py\", line 249, in combine_docs\n",
      "    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 318, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 128, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 140, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 714, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 571, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 561, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 793, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 286, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 217, in _chat_stream_with_aggregation\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 189, in _create_chat_stream\n",
      "    yield from self._create_stream(\n",
      "               ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 265, in _create_stream\n",
      "    raise OllamaEndpointNotFoundError(\n",
      "langchain_community.llms.ollama.OllamaEndpointNotFoundError: Ollama call failed with status code 404. Maybe your model is not found and you should pull the model with `ollama pull meta-llama-3.1`.\n",
      "127.0.0.1 - - [28/Aug/2024 14:50:30] \"POST /summary HTTP/1.1\" 500 -\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from docx import Document\n",
    "from flask import *\n",
    "from flask_cors import CORS\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "app = Flask(__name__)\n",
    "# CORS(app, resources={r'*': {'origins': 'http://localhost:8000'}})\n",
    "CORS(app)\n",
    "host = 'localhost'\n",
    "port = 5100\n",
    "\n",
    "# Configuration\n",
    "ngrok = 'localhost:11434'\n",
    "# ngrok = 'https://6c82-35-229-167-67.ngrok-free.app'\n",
    "device = 'cpu'\n",
    "\n",
    "# Load language model\n",
    "llm_model = ChatOllama(\n",
    "    model='meta-llama-3.1',\n",
    "    # base_url=ngrok\n",
    ")\n",
    "\n",
    "# Load embeddings model\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# Define prompt\n",
    "prompt_template = '''Use the following pieces of context to answer the question at the end.\n",
    "If you don't find the answer in context, don't try to make up an answer.\n",
    "If you find the answer in context, answer me only use korean.\n",
    "\n",
    "context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:'''\n",
    "rag_prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Function to create the DOCX file\n",
    "def create_docx(response, vectorstore):\n",
    "    doc = Document()\n",
    "    title = response.get('title', '제목 없음')\n",
    "    doc.add_heading(title, level=0)\n",
    "\n",
    "    doc.add_heading('목차', level=1)\n",
    "    doc.add_paragraph('1. 개요')\n",
    "    doc.add_paragraph('2. 본문')\n",
    "    for n, cont in enumerate(response.get('content', []), 1):\n",
    "        doc.add_paragraph(f'\\t2-{n}. {cont}')\n",
    "\n",
    "    doc.add_heading('1. 개요', level=1)\n",
    "    doc.add_paragraph(response.get('summary', ''))\n",
    "\n",
    "    doc.add_heading('2. 본문', level=1)\n",
    "    for n, cont in enumerate(response.get('content', []), 1):\n",
    "        question = f'{title}라는 보고서의 {cont} 부분 상세 내용을 글로 풀어서 적어줘'    # {title}라는 보고서의 {cont} 부분 상세 내용 markdown 형식으로\n",
    "        memory = ConversationBufferMemory(\n",
    "            memory_key='chat_history',\n",
    "            return_messages=True\n",
    "        )\n",
    "        conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "            llm=llm_model,\n",
    "            retriever=vectorstore.as_retriever(),\n",
    "            condense_question_prompt=rag_prompt,\n",
    "            memory=memory\n",
    "        )\n",
    "        res = conversation_chain({'question': question})\n",
    "        doc.add_heading(f'\\t2-{n}. {cont}', level=2)\n",
    "        doc.add_paragraph(res['chat_history'][1].content)\n",
    "    \n",
    "    # Format date to avoid issues in filenames\n",
    "    doc.save(f'./processed/{datetime.datetime.today().strftime(\"%g%m%d%H%M%S\")}.docx')\n",
    "    return f'/processed/{datetime.datetime.today().strftime(\"%g%m%d%H%M%S\")}.docx'\n",
    "\n",
    "def process_file(file_path):\n",
    "    # if not os.path.isfile(file_path):\n",
    "    #     print(f'파일이 존재하지 않습니다: {file_path}')\n",
    "    #     return\n",
    "    \n",
    "    text_sum = ''\n",
    "    # files = [file_path]\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    for file in file_path:\n",
    "        reader = PdfReader(file)\n",
    "        for page in reader.pages:  # 페이지 별로 텍스트 추출\n",
    "            text = page.extract_text()\n",
    "            corrected_text = text.encode('utf-8', errors='ignore').decode('utf-8')  # 인코딩 오류 무시 및 텍스트 누적\n",
    "            text_sum += corrected_text + '\\n'\n",
    "    splits = text_splitter.split_text(text_sum)\n",
    "    print(len(splits))\n",
    "    if len(splits) == 0: \n",
    "        return False\n",
    "    # Create FAISS index\n",
    "    vectorstore = FAISS.from_texts(splits, embedding_model)\n",
    "    \n",
    "    # Assuming text processing and JSON generation happens here\n",
    "    # For demonstration, we're creating a sample response\n",
    "    response = {\n",
    "        'title': '보고서 제목',\n",
    "        'content': ['목차 1', '목차 2'],\n",
    "        'summary': '보고서 개요'\n",
    "    }\n",
    "\n",
    "    return create_docx(response, vectorstore)\n",
    "\n",
    "@app.route('/summary', methods=['POST'])\n",
    "def summary():\n",
    "    \n",
    "    files = [request.files[i] for i in request.files]\n",
    "    processedFilePath = process_file(files)\n",
    "    \n",
    "    if processedFilePath:\n",
    "        return jsonify({\n",
    "            'result': 'ok',\n",
    "            'processedFilePath': processedFilePath\n",
    "        })\n",
    "    else:\n",
    "        return jsonify({\n",
    "            'result': 'error',\n",
    "            'error': '텍스트를 인식할 수 없음'\n",
    "        })\n",
    "if __name__ == '__main__':\n",
    "    app.run(host= host, port=port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flask_cors\n",
      "  Downloading Flask_Cors-4.0.1-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: Flask>=0.9 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from flask_cors) (2.2.5)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from Flask>=0.9->flask_cors) (2.2.3)\n",
      "Requirement already satisfied: Jinja2>=3.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from Flask>=0.9->flask_cors) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from Flask>=0.9->flask_cors) (2.0.1)\n",
      "Requirement already satisfied: click>=8.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from Flask>=0.9->flask_cors) (8.1.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from click>=8.0->Flask>=0.9->flask_cors) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from Jinja2>=3.0->Flask>=0.9->flask_cors) (2.1.3)\n",
      "Downloading Flask_Cors-4.0.1-py2.py3-none-any.whl (14 kB)\n",
      "Installing collected packages: flask_cors\n",
      "Successfully installed flask_cors-4.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install flask_cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
