{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://localhost:5100\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [28/Aug/2024 14:48:09] \"POST /summary HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [28/Aug/2024 14:49:35] \"POST /summary HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [28/Aug/2024 14:50:00] \"POST /summary HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [28/Aug/2024 14:50:06] \"POST /summary HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "60\n",
      "60\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-28 14:52:46,347] ERROR in app: Exception on /summary [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 2529, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 1825, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\flask_cors\\extension.py\", line 178, in wrapped_function\n",
      "    return cors_after_request(app.make_response(f(*args, **kwargs)))\n",
      "                                                ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 1823, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 1799, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SMHRD\\AppData\\Local\\Temp\\ipykernel_10980\\4098571896.py\", line 126, in summary\n",
      "    processedFilePath = process_file(files)\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SMHRD\\AppData\\Local\\Temp\\ipykernel_10980\\4098571896.py\", line 120, in process_file\n",
      "    return create_docx(response, vectorstore)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SMHRD\\AppData\\Local\\Temp\\ipykernel_10980\\4098571896.py\", line 82, in create_docx\n",
      "    res = conversation_chain({'question': question})\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\conversational_retrieval\\base.py\", line 167, in _call\n",
      "    answer = self.combine_docs_chain.run(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 605, in run\n",
      "    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\combine_documents\\base.py\", line 137, in _call\n",
      "    output, extra_return_dict = self.combine_docs(\n",
      "                                ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\combine_documents\\stuff.py\", line 244, in combine_docs\n",
      "    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 317, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 127, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\chains\\llm.py\", line 139, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 286, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 217, in _chat_stream_with_aggregation\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 189, in _create_chat_stream\n",
      "    yield from self._create_stream(\n",
      "               ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 232, in _create_stream\n",
      "    response = requests.post(\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\requests\\api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\requests\\sessions.py\", line 697, in send\n",
      "    adapter = self.get_adapter(url=request.url)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\requests\\sessions.py\", line 794, in get_adapter\n",
      "    raise InvalidSchema(f\"No connection adapters were found for {url!r}\")\n",
      "requests.exceptions.InvalidSchema: No connection adapters were found for 'localhost:11434/api/chat'\n",
      "127.0.0.1 - - [28/Aug/2024 14:52:46] \"POST /summary HTTP/1.1\" 500 -\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from docx import Document\n",
    "from flask import *\n",
    "from flask_cors import CORS\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "app = Flask(__name__)\n",
    "# CORS(app, resources={r'*': {'origins': 'http://localhost:8000'}})\n",
    "CORS(app)\n",
    "host = 'localhost'\n",
    "port = 5100\n",
    "\n",
    "# Configuration\n",
    "ngrok = 'localhost:11434'\n",
    "# ngrok = 'https://6c82-35-229-167-67.ngrok-free.app'\n",
    "device = 'cpu'\n",
    "\n",
    "# Load language model\n",
    "llm_model = ChatOllama(\n",
    "    model='meta-llama-3.1',\n",
    "    # base_url=ngrok\n",
    ")\n",
    "\n",
    "# Load embeddings model\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# Define prompt\n",
    "prompt_template = '''Use the following pieces of context to answer the question at the end.\n",
    "If you don't find the answer in context, don't try to make up an answer.\n",
    "If you find the answer in context, answer me only use korean.\n",
    "\n",
    "context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:'''\n",
    "rag_prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Function to create the DOCX file\n",
    "def create_docx(response, vectorstore):\n",
    "    doc = Document()\n",
    "    title = response.get('title', '제목 없음')\n",
    "    doc.add_heading(title, level=0)\n",
    "\n",
    "    doc.add_heading('목차', level=1)\n",
    "    doc.add_paragraph('1. 개요')\n",
    "    doc.add_paragraph('2. 본문')\n",
    "    for n, cont in enumerate(response.get('content', []), 1):\n",
    "        doc.add_paragraph(f'\\t2-{n}. {cont}')\n",
    "\n",
    "    doc.add_heading('1. 개요', level=1)\n",
    "    doc.add_paragraph(response.get('summary', ''))\n",
    "\n",
    "    doc.add_heading('2. 본문', level=1)\n",
    "    for n, cont in enumerate(response.get('content', []), 1):\n",
    "        question = f'{title}라는 보고서의 {cont} 부분 상세 내용을 글로 풀어서 적어줘'    # {title}라는 보고서의 {cont} 부분 상세 내용 markdown 형식으로\n",
    "        memory = ConversationBufferMemory(\n",
    "            memory_key='chat_history',\n",
    "            return_messages=True\n",
    "        )\n",
    "        conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "            llm=llm_model,\n",
    "            retriever=vectorstore.as_retriever(),\n",
    "            condense_question_prompt=rag_prompt,\n",
    "            memory=memory\n",
    "        )\n",
    "        res = conversation_chain({'question': question})\n",
    "        doc.add_heading(f'\\t2-{n}. {cont}', level=2)\n",
    "        doc.add_paragraph(res['chat_history'][1].content)\n",
    "    \n",
    "    # Format date to avoid issues in filenames\n",
    "    doc.save(f'./processed/{datetime.datetime.today().strftime(\"%g%m%d%H%M%S\")}.docx')\n",
    "    return f'/processed/{datetime.datetime.today().strftime(\"%g%m%d%H%M%S\")}.docx'\n",
    "\n",
    "def process_file(file_path):\n",
    "    # if not os.path.isfile(file_path):\n",
    "    #     print(f'파일이 존재하지 않습니다: {file_path}')\n",
    "    #     return\n",
    "    \n",
    "    text_sum = ''\n",
    "    # files = [file_path]\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    for file in file_path:\n",
    "        reader = PdfReader(file)\n",
    "        for page in reader.pages:  # 페이지 별로 텍스트 추출\n",
    "            text = page.extract_text()\n",
    "            corrected_text = text.encode('utf-8', errors='ignore').decode('utf-8')  # 인코딩 오류 무시 및 텍스트 누적\n",
    "            text_sum += corrected_text + '\\n'\n",
    "    splits = text_splitter.split_text(text_sum)\n",
    "    print(len(splits))\n",
    "    if len(splits) == 0: \n",
    "        return False\n",
    "    # Create FAISS index\n",
    "    vectorstore = FAISS.from_texts(splits, embedding_model)\n",
    "    \n",
    "    # Assuming text processing and JSON generation happens here\n",
    "    # For demonstration, we're creating a sample response\n",
    "    response = {\n",
    "        'title': '보고서 제목',\n",
    "        'content': ['목차 1', '목차 2'],\n",
    "        'summary': '보고서 개요'\n",
    "    }\n",
    "\n",
    "    return create_docx(response, vectorstore)\n",
    "\n",
    "@app.route('/summary', methods=['POST'])\n",
    "def summary():\n",
    "    \n",
    "    files = [request.files[i] for i in request.files]\n",
    "    processedFilePath = process_file(files)\n",
    "    \n",
    "    if processedFilePath:\n",
    "        return jsonify({\n",
    "            'result': 'ok',\n",
    "            'processedFilePath': processedFilePath\n",
    "        })\n",
    "    else:\n",
    "        return jsonify({\n",
    "            'result': 'error',\n",
    "            'error': '텍스트를 인식할 수 없음'\n",
    "        })\n",
    "if __name__ == '__main__':\n",
    "    app.run(host= host, port=port)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
