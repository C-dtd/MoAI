{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from docx import Document\n",
    "from flask import *\n",
    "from flask_cors import CORS\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "app = Flask(__name__)\n",
    "# CORS(app, resources={r'*': {'origins': 'http://localhost:8000'}})\n",
    "CORS(app)\n",
    "host = 'localhost'\n",
    "port = 5100\n",
    "\n",
    "# Configuration\n",
    "ngrok = 'https://6c82-35-229-167-67.ngrok-free.app'  # Replace with your ngrok URL\n",
    "device = 'cpu'\n",
    "\n",
    "# Load language model\n",
    "llm_model = ChatOllama(\n",
    "    model='meta-llama-3.1',\n",
    "    base_url=ngrok\n",
    ")\n",
    "\n",
    "# Load embeddings model\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# Define prompt\n",
    "prompt_template = '''Use the following pieces of context to answer the question at the end.\n",
    "If you don't find the answer in context, don't try to make up an answer.\n",
    "If you find the answer in context, answer me only use korean.\n",
    "\n",
    "context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:'''\n",
    "rag_prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Function to create the DOCX file\n",
    "def create_docx(response, vectorstore):\n",
    "    doc = Document()\n",
    "    title = response.get('title', '제목 없음')\n",
    "    doc.add_heading(title, level=0)\n",
    "\n",
    "    doc.add_heading('목차', level=1)\n",
    "    doc.add_paragraph('1. 개요')\n",
    "    doc.add_paragraph('2. 본문')\n",
    "    for n, cont in enumerate(response.get('content', []), 1):\n",
    "        doc.add_paragraph(f'\\t2-{n}. {cont}')\n",
    "\n",
    "    doc.add_heading('1. 개요', level=1)\n",
    "    doc.add_paragraph(response.get('summary', ''))\n",
    "\n",
    "    doc.add_heading('2. 본문', level=1)\n",
    "    for n, cont in enumerate(response.get('content', []), 1):\n",
    "        question = f'{title}라는 보고서의 {cont} 부분 상세 내용을 글로 풀어서 적어줘'    # {title}라는 보고서의 {cont} 부분 상세 내용 markdown 형식으로\n",
    "        memory = ConversationBufferMemory(\n",
    "            memory_key='chat_history',\n",
    "            return_messages=True\n",
    "        )\n",
    "        conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "            llm=llm_model,\n",
    "            retriever=vectorstore.as_retriever(),\n",
    "            condense_question_prompt=rag_prompt,\n",
    "            memory=memory\n",
    "        )\n",
    "        res = conversation_chain({'question': question})\n",
    "        doc.add_heading(f'\\t2-{n}. {cont}', level=2)\n",
    "        doc.add_paragraph(res['chat_history'][1].content)\n",
    "    \n",
    "    # Format date to avoid issues in filenames\n",
    "    doc.save(f'./processed/{datetime.datetime.today().strftime(\"%g%m%d%H%M%S\")}.docx')\n",
    "    return f'/processed/{datetime.datetime.today().strftime(\"%g%m%d%H%M%S\")}.docx'\n",
    "\n",
    "def process_file(file_path):\n",
    "    # if not os.path.isfile(file_path):\n",
    "    #     print(f'파일이 존재하지 않습니다: {file_path}')\n",
    "    #     return\n",
    "    \n",
    "    text_sum = ''\n",
    "    # files = [file_path]\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    for file in file_path:\n",
    "        reader = PdfReader(file)\n",
    "        for page in reader.pages:  # 페이지 별로 텍스트 추출\n",
    "            text = page.extract_text()\n",
    "            corrected_text = text.encode('utf-8', errors='ignore').decode('utf-8')  # 인코딩 오류 무시 및 텍스트 누적\n",
    "            text_sum += corrected_text + '\\n'\n",
    "    splits = text_splitter.split_text(text_sum)\n",
    "\n",
    "    # Create FAISS index\n",
    "    vectorstore = FAISS.from_texts(splits, embedding_model)\n",
    "    \n",
    "    # Assuming text processing and JSON generation happens here\n",
    "    # For demonstration, we're creating a sample response\n",
    "    response = {\n",
    "        'title': '보고서 제목',\n",
    "        'content': ['목차 1', '목차 2'],\n",
    "        'summary': '보고서 개요'\n",
    "    }\n",
    "\n",
    "    return create_docx(response, vectorstore)\n",
    "\n",
    "@app.route('/summary', methods=['POST'])\n",
    "def summary():\n",
    "    \n",
    "    files = [request.files[i] for i in request.files]\n",
    "    processedFilePath = process_file(files)\n",
    "    return jsonify({\n",
    "        'result': 'ok',\n",
    "        'processedFilePath': processedFilePath\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host= host, port=port)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
