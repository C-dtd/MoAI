{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://localhost:5100\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from docx import Document\n",
    "from flask import *\n",
    "from flask_cors import CORS\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "app = Flask(__name__)\n",
    "# CORS(app, resources={r'*': {'origins': 'http://localhost:8000'}})\n",
    "CORS(app)\n",
    "host = 'localhost'\n",
    "port = 5100\n",
    "\n",
    "# Configuration\n",
    "ngrok = 'https://6c82-35-229-167-67.ngrok-free.app'\n",
    "device = 'cpu'\n",
    "\n",
    "# Load language model\n",
    "llm_model = ChatOllama(\n",
    "    model='meta-llama-3.1',\n",
    "    # base_url=ngrok\n",
    ")\n",
    "llm_model_json = ChatOllama(\n",
    "    model='meta-llama-3.1',\n",
    "    format='json',\n",
    "    # base_url=ngrok\n",
    ")\n",
    "\n",
    "# Load embeddings model\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# Define prompt\n",
    "prompt_template = '''Use the following pieces of context to answer the question at the end.\n",
    "If you don't find the answer in context, don't try to make up an answer.\n",
    "If you find the answer in context, answer me only use korean.\n",
    "\n",
    "context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:'''\n",
    "rag_prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Function to create the DOCX file\n",
    "def create_docx(response, vectorstore):\n",
    "    doc = Document()\n",
    "    title = response.get('title', '제목 없음')\n",
    "    doc.add_heading(title, level=0)\n",
    "\n",
    "    doc.add_heading('목차', level=1)\n",
    "    doc.add_paragraph('1. 개요')\n",
    "    doc.add_paragraph('2. 본문')\n",
    "    for n, cont in enumerate(response.get('content', []), 1):\n",
    "        doc.add_paragraph(f'\\t2-{n}. {cont}')\n",
    "\n",
    "    doc.add_heading('1. 개요', level=1)\n",
    "    doc.add_paragraph(response.get('summary', ''))\n",
    "\n",
    "    doc.add_heading('2. 본문', level=1)\n",
    "    for n, cont in enumerate(response.get('content', []), 1):\n",
    "        question = f'제목이 \"{title}\"인 보고서의 \"{cont}\" 부분 상세 내용을 글로 풀어서 적어줘'    # {title}라는 보고서의 {cont} 부분 상세 내용 markdown 형식으로\n",
    "        memory = ConversationBufferMemory(\n",
    "            memory_key='chat_history',\n",
    "            return_messages=True\n",
    "        )\n",
    "        conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "            llm=llm_model,\n",
    "            retriever=vectorstore.as_retriever(),\n",
    "            condense_question_prompt=rag_prompt,\n",
    "            memory=memory\n",
    "        )\n",
    "        res = conversation_chain({'question': question})\n",
    "        doc.add_heading(f'\\t2-{n}. {cont}', level=2)\n",
    "        doc.add_paragraph(res['chat_history'][1].content)\n",
    "    \n",
    "    # Format date to avoid issues in filenames\n",
    "    doc.save(f'./processed/{datetime.datetime.today().strftime(\"%g%m%d%H%M%S\")}.docx')\n",
    "    return f'/processed/{datetime.datetime.today().strftime(\"%g%m%d%H%M%S\")}.docx'\n",
    "\n",
    "def process_file(file_path):\n",
    "    # if not os.path.isfile(file_path):\n",
    "    #     print(f'파일이 존재하지 않습니다: {file_path}')\n",
    "    #     return\n",
    "    \n",
    "    text_sum = ''\n",
    "    # files = [file_path]\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    for file in file_path:\n",
    "        reader = PdfReader(file)\n",
    "        for page in reader.pages:  # 페이지 별로 텍스트 추출\n",
    "            text = page.extract_text()\n",
    "            corrected_text = text.encode('utf-8', errors='ignore').decode('utf-8')  # 인코딩 오류 무시 및 텍스트 누적\n",
    "            text_sum += corrected_text + '\\n'\n",
    "    splits = text_splitter.split_text(text_sum)\n",
    "    print(len(splits))\n",
    "    if len(splits) == 0: \n",
    "        return False\n",
    "    # Create FAISS index\n",
    "    vectorstore = FAISS.from_texts(splits, embedding_model)\n",
    "    \n",
    "    \n",
    "    question = '''문서 요약 관련 보고서를 만들어줘 \n",
    "        'title': '보고서의 제목',\n",
    "        'content':list['보고서의 목차별 제목'] 20글자 이내,\n",
    "        'summary': '보고서의 개요' 1000글자 이내\n",
    "    key is title, content, summary.\n",
    "    Respond using JSON only.'''\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key='chat_history',\n",
    "        return_messages=True,\n",
    "    )\n",
    "\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm_model_json,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        condense_question_prompt=rag_prompt,\n",
    "        memory=memory,\n",
    "    )\n",
    "    res = conversation_chain({'question': question})\n",
    "    response = res['chat_history'][1].content.replace('\\n', '').lstrip().rstrip()\n",
    "    response = json.loads(response)\n",
    "\n",
    "    return create_docx(response, vectorstore)\n",
    "\n",
    "@app.route('/summary', methods=['POST'])\n",
    "def summary():\n",
    "    \n",
    "    files = [request.files[i] for i in request.files]\n",
    "    processedFilePath = process_file(files)\n",
    "    \n",
    "    if processedFilePath:\n",
    "        return jsonify({\n",
    "            'result': 'ok',\n",
    "            'processedFilePath': processedFilePath\n",
    "        })\n",
    "    else:\n",
    "        return jsonify({\n",
    "            'result': 'error',\n",
    "            'error': '텍스트를 인식할 수 없음'\n",
    "        })\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    app.run(host= host, port=port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flask_cors\n",
      "  Downloading Flask_Cors-4.0.1-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: Flask>=0.9 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from flask_cors) (2.2.5)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from Flask>=0.9->flask_cors) (2.2.3)\n",
      "Requirement already satisfied: Jinja2>=3.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from Flask>=0.9->flask_cors) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from Flask>=0.9->flask_cors) (2.0.1)\n",
      "Requirement already satisfied: click>=8.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from Flask>=0.9->flask_cors) (8.1.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from click>=8.0->Flask>=0.9->flask_cors) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from Jinja2>=3.0->Flask>=0.9->flask_cors) (2.1.3)\n",
      "Downloading Flask_Cors-4.0.1-py2.py3-none-any.whl (14 kB)\n",
      "Installing collected packages: flask_cors\n",
      "Successfully installed flask_cors-4.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install flask_cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
