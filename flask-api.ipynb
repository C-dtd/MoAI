{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://localhost:5100\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [05/Sep/2024 09:59:41] \"OPTIONS /chat HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [05/Sep/2024 10:00:07] \"POST /chat HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from docx import Document\n",
    "from flask import *\n",
    "from flask_cors import CORS\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "import psycopg2\n",
    "from psycopg2 import pool\n",
    "\n",
    "dbcp = psycopg2.pool.SimpleConnectionPool(1, 20,\n",
    "    user='postgres.vpcdvbdktvvzrvjfyyzm',\n",
    "    password='Odvv8E1iChKjwai4',\n",
    "    host='aws-0-ap-southeast-1.pooler.supabase.com',\n",
    "    port=6543,\n",
    "    dbname='postgres'\n",
    ")\n",
    "\n",
    "app = Flask(__name__)\n",
    "# CORS(app, resources={r'*': {'origins': 'http://localhost:8000'}})\n",
    "CORS(app)\n",
    "host = 'localhost'\n",
    "port = 5100\n",
    "\n",
    "connection='postgresql+psycopg2://postgres.vpcdvbdktvvzrvjfyyzm:Odvv8E1iChKjwai4@aws-0-ap-southeast-1.pooler.supabase.com:6543/postgres'\n",
    "\n",
    "# Configuration\n",
    "ngrok = ''\n",
    "ngrok = 'https://b1f9-35-197-57-104.ngrok-free.app'\n",
    "device = 'cpu'\n",
    "\n",
    "if ngrok == '':\n",
    "    llm_model = ChatOllama(\n",
    "        model='meta-llama-3.1',\n",
    "    )\n",
    "    llm_model_json = ChatOllama(\n",
    "        model='meta-llama-3.1',\n",
    "        format='json', \n",
    "    )\n",
    "else:\n",
    "    # Load language model\n",
    "    llm_model = ChatOllama(\n",
    "        model='meta-llama-3.1',\n",
    "        base_url=ngrok      # 주석 해제 시 코랩 자원으로 돌아감, # 주석 설정 시 로컬 자원으로 돌아감 \n",
    "    )\n",
    "    llm_model_json = ChatOllama(\n",
    "        model='meta-llama-3.1',\n",
    "        format='json', \n",
    "        base_url=ngrok       # 주석 해제 시 코랩 자원으로 돌아감, # 주석 설정 시 로컬 자원으로 돌아감 \n",
    "    )\n",
    "\n",
    "# Load embeddings model\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Define prompt\n",
    "prompt_template = '''Use the following pieces of context to answer the question at the end.\n",
    "If you don't find the answer in context, don't try to make up an answer.\n",
    "If you find the answer in context, answer me only use korean.\n",
    "\n",
    "context: {chat_history}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:'''\n",
    "# rag_prompt = PromptTemplate.from_template(prompt_template)\n",
    "rag_prompt = PromptTemplate(template=prompt_template, input_variables=['chat_history', 'question'])\n",
    "\n",
    "prompt_template = '''Use the following pieces of context to answer the question at the end.\n",
    "If you don't find the answer in context, don't try to make up an answer.\n",
    "If you find the answer in context, answer me only use korean.\n",
    "\n",
    "context: {chat_history}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:'''\n",
    "# rag_prompt = PromptTemplate.from_template(prompt_template)\n",
    "chat_prompt = PromptTemplate(template=prompt_template, input_variables=['chat_history', 'question'])\n",
    "\n",
    "\n",
    "\n",
    "#db에서 접근 권한이 있는 정보를 문장으로 만들어 분할하는 함수\n",
    "def db_crawler(user_id):\n",
    "    db = dbcp.getconn()\n",
    "    cur = db.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "                select user_name, job_id, dep_name \n",
    "                from users u join departments d on u.dep_id = d.dep_id\n",
    "                where user_id=%s\n",
    "                \"\"\",\n",
    "                (user_id,))\n",
    "    user_info = cur.fetchone()\n",
    "    cur.execute(\"\"\"\n",
    "                select cl.chat_at, u.user_name, cl.cl_chat\n",
    "                from chat_logs cl join users u on cl.user_id = u.user_id\n",
    "                where cl_type = 'text' and room_id in (select room_id from room_users where user_id = %s)\n",
    "                order by chat_at\n",
    "                \"\"\",\n",
    "                (user_id,))\n",
    "    chat_rows = cur.fetchall()\n",
    "    cur.execute(\"\"\"\n",
    "                select cal_title, cal_start_date, cal_end_date, cal_location\n",
    "                from calendars\n",
    "                where user_id = %s or cal_id in (select cal_sub_id from calendar_shared)\n",
    "                order by cal_start_date\n",
    "                \"\"\",\n",
    "                (user_id,))\n",
    "    cal_rows = cur.fetchall()\n",
    "    cur.execute(\"\"\"\n",
    "                select user_name, job_id, dep_name, user_phone\n",
    "                from users u join departments d on u.dep_id = d.dep_id\n",
    "                \"\"\")\n",
    "    member_rows = cur.fetchall()\n",
    "    \n",
    "    dbcp.putconn(db)\n",
    "\n",
    "    db_crawls = f'사용자 이름: {user_info[0]}, 사용자 직책: {user_info[1]}, 사용자 부서: {user_info[2]}'\n",
    "    db_crawls += datetime.datetime.now().strftime('오늘은 %G-%m-%d, %A입니다.')\n",
    "\n",
    "    for row in chat_rows:\n",
    "        _ = '\\n'\n",
    "        db_crawls += f\"{row[0].strftime('%G-%m-%d %T')}에 {row[1]}이(가) '{row[2].replace(_, '')}'라 말함. \"\n",
    "    for row in cal_rows:\n",
    "        db_crawls += f\"{row[1].replace('T', ' ')}부터 {row[2].replace('T', ' ')}까지 일정: '{row[0]}'이 {row[3] +'에서 ' if row[3] else ''}있습니다. \"\n",
    "    for row in member_rows:\n",
    "        db_crawls += f\"{row[0]}은 {row[2]} 부서의 {row[1]} 직책을 담당하고 있습니다. 연락처는 {row[3]} 입니다.\"\n",
    "    \n",
    "    \n",
    "    db_split = text_splitter.split_text(db_crawls)\n",
    "    \n",
    "    return db_split\n",
    "\n",
    "def process_file(file_path, user_input, user_id):\n",
    "    text_sum = ''\n",
    "    db_split = db_crawler(user_id)\n",
    "\n",
    "    for file in file_path:\n",
    "        try:\n",
    "            reader = PdfReader(file)\n",
    "            for page in reader.pages:  # 페이지 별로 텍스트 추출\n",
    "                text = page.extract_text()\n",
    "                corrected_text = text.encode('utf-8', errors='ignore').decode('utf-8')  # 인코딩 오류 무시 및 텍스트 누적\n",
    "                text_sum += corrected_text + '\\n'\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    splits = text_splitter.split_text(text_sum)\n",
    "    splits += db_split\n",
    "    # if len(splits) == 0: \n",
    "    #         return False\n",
    "        \n",
    "    # Create FAISS index\n",
    "    vectorstore = FAISS.from_texts(splits, embedding_model)\n",
    "    \n",
    "    question = f'사용자의 입력: {user_input}'+'''\n",
    "    사용자의 입력을 보고 주제를 정해서 문서 요약 관련 보고서를 만들어줘\n",
    "    {\n",
    "        'title': '보고서의 제목',\n",
    "        'content':list['보고서의 목차별 제목'] 20글자 이내,\n",
    "        'summary': '보고서의 개요' 1000글자 이내\n",
    "    }\n",
    "    key is title, content, summary.\n",
    "    Respond using JSON only.'''\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key='chat_history',\n",
    "        return_messages=True,\n",
    "    )\n",
    "\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm_model_json,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        condense_question_prompt=rag_prompt,\n",
    "        memory=memory,\n",
    "    )\n",
    "    res = conversation_chain({'question': question})\n",
    "    response = res['chat_history'][1].content.replace('\\n', '').lstrip().rstrip()\n",
    "    response = json.loads(response)\n",
    "\n",
    "    return create_docx(response, vectorstore)\n",
    "\n",
    "# Function to create the DOCX file\n",
    "def create_docx(response, vectorstore):\n",
    "    doc = Document()\n",
    "    title = response.get('title', '제목 없음')\n",
    "    doc.add_heading(title, level=0)\n",
    "\n",
    "    doc.add_heading('목차', level=1)\n",
    "    doc.add_paragraph('1. 개요')\n",
    "    doc.add_paragraph('2. 본문')\n",
    "    for n, cont in enumerate(response.get('content', []), 1):\n",
    "        doc.add_paragraph(f'\\t2-{n}. {cont}')\n",
    "\n",
    "    doc.add_heading('1. 개요', level=1)\n",
    "    doc.add_paragraph(response.get('summary', ''))\n",
    "\n",
    "    doc.add_heading('2. 본문', level=1)\n",
    "    for n, cont in enumerate(response.get('content', []), 1):\n",
    "        question = f'제목이 \"{title}\"인 보고서의 \"{cont}\" 부분 상세 내용을 글로 풀어서 적어줘'    # {title}라는 보고서의 {cont} 부분 상세 내용 markdown 형식으로\n",
    "        memory = ConversationBufferMemory(\n",
    "            memory_key='chat_history',\n",
    "            return_messages=True\n",
    "        )\n",
    "        conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "            llm=llm_model,\n",
    "            retriever=vectorstore.as_retriever(),\n",
    "            condense_question_prompt=rag_prompt,\n",
    "            memory=memory\n",
    "        )\n",
    "        res = conversation_chain({'question': question})\n",
    "        doc.add_heading(f'\\t2-{n}. {cont}', level=2)\n",
    "        doc.add_paragraph(res['chat_history'][1].content)\n",
    "    \n",
    "    # Format date to avoid issues in filenames\n",
    "    doc.save(f'./processed/{datetime.datetime.today().strftime(\"%g%m%d%H%M%S\")}.docx')\n",
    "    return f'/processed/{datetime.datetime.today().strftime(\"%g%m%d%H%M%S\")}.docx'\n",
    "\n",
    "@app.route('/summary', methods=['POST'])\n",
    "def summary():\n",
    "    user_input = request.form.get('inputText', '').strip()\n",
    "    user_id = request.form.get('user_id')\n",
    "    files = [request.files[i] for i in request.files]\n",
    "    processedFilePath = process_file(files, user_input, user_id)\n",
    "    \n",
    "    if processedFilePath:\n",
    "        return jsonify({\n",
    "            'result': 'ok',\n",
    "            'processedFilePath': processedFilePath\n",
    "        })\n",
    "    else:\n",
    "        return jsonify({\n",
    "            'result': 'error',\n",
    "            'error': '텍스트를 인식할 수 없음'\n",
    "        })\n",
    "\n",
    "@app.route('/chat', methods=['POST'])\n",
    "def chat():\n",
    "    params = request.get_json()\n",
    "    user_input = params.get('user_input')\n",
    "    user_id = params.get('user_id')\n",
    "    # print(params)\n",
    "    splits = db_crawler(user_id)\n",
    "    vectorstore = FAISS.from_texts(splits, embedding_model)\n",
    "\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key='chat_history',\n",
    "        return_messages=True\n",
    "    )\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm_model,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        condense_question_prompt=chat_prompt,\n",
    "        memory=memory\n",
    "    )\n",
    "    \n",
    "    conn = dbcp.getconn()\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    cur.execute(\n",
    "        \"select ac_question, ac_answer from ai_chat_logs where user_id=%s order by chat_at asc\",\n",
    "        (user_id,)\n",
    "    )\n",
    "    rows = cur.fetchall()\n",
    "    \n",
    "    cur.close()\n",
    "    dbcp.putconn(conn)\n",
    "    \n",
    "    for row in rows:\n",
    "        memory.save_context(\n",
    "            inputs={'human': row[0]},\n",
    "            outputs={'ai': row[1]}\n",
    "        )\n",
    "        \n",
    "    res = conversation_chain({'question': user_input})\n",
    "    \n",
    "    conn = dbcp.getconn()\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    cur.execute(\n",
    "        \"insert into ai_chat_logs (user_id, ac_question, ac_answer) values (%s, %s, %s)\",\n",
    "        (user_id, user_input, res['answer'])\n",
    "    )\n",
    "    conn.commit()\n",
    "    \n",
    "    cur.close()\n",
    "    dbcp.putconn(conn)\n",
    "    return jsonify({'answer': res['answer']})\n",
    "       \n",
    "if __name__ == '__main__':\n",
    "    app.run(host= host, port=port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @app.route('/generate_report', methods=['POST'])\n",
    "# def generate_report():\n",
    "#     # 사용자가 입력한 텍스트 가져오기\n",
    "#     input_text = request.form.get('inputText', '').strip()\n",
    "#     user_id = request.form.get('user_id', '').strip()\n",
    "    \n",
    "#     # if not input_text:\n",
    "#     #     return jsonify({\n",
    "#     #         'result': 'error',\n",
    "#     #         'error': '입력된 텍스트가 없습니다.'\n",
    "#     #     })\n",
    "\n",
    "#     # 텍스트를 분할하여 벡터스토어 생성\n",
    "#     db_split = db_crawler(user_id)\n",
    "#     # splits = text_splitter.split_text(input_text)\n",
    "\n",
    "#     if len(db_split) == 0: \n",
    "#         return jsonify({\n",
    "#             'result': 'error',\n",
    "#             'error': '텍스트 분할에 실패했습니다.'\n",
    "#         })\n",
    "    \n",
    "#     # Create FAISS index from input text splits\n",
    "#     vectorstore = FAISS.from_texts(db_split, embedding_model)\n",
    "\n",
    "#     # 보고서 생성 요청 프롬프트\n",
    "#     question = f'''사용자의 입력: {input_text} \n",
    "#     사용자의 입력을 읽고 관련 보고서를 만들어줘\n",
    "#         'title': '보고서의 제목',\n",
    "#         'content':list['보고서의 목차별 제목'] 50글자 이내,\n",
    "#         'summary': '보고서의 개요' 1000글자 이내\n",
    "#     key is title, content, summary.\n",
    "#     Respond using JSON only.'''\n",
    "\n",
    "#     # 메모리 설정 및 대화형 체인 생성\n",
    "#     memory = ConversationBufferMemory(\n",
    "#         memory_key='chat_history',\n",
    "#         return_messages=True,\n",
    "#     )\n",
    "\n",
    "#     conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "#         llm=llm_model_json,\n",
    "#         retriever=vectorstore.as_retriever(),\n",
    "#         condense_question_prompt=rag_prompt,\n",
    "#         memory=memory,\n",
    "#     )\n",
    "\n",
    "#     # 보고서 생성\n",
    "#     res = conversation_chain({'question': question})\n",
    "#     response_content = res['chat_history'][1].content.replace('\\n', '').lstrip().rstrip()\n",
    "\n",
    "#     try:\n",
    "#         # JSON 형식으로 파싱\n",
    "#         response = json.loads(response_content)\n",
    "#     except json.JSONDecodeError:\n",
    "#         return jsonify({\n",
    "#             'result': 'error',\n",
    "#             'error': '보고서 생성 중 JSON 파싱 오류가 발생했습니다.'\n",
    "#         })\n",
    "    \n",
    "#     # DOCX 파일 생성\n",
    "#     processedFilePath = create_docx(response, vectorstore)\n",
    "    \n",
    "#     # 결과 반환\n",
    "#     if processedFilePath:\n",
    "#         return jsonify({\n",
    "#             'result': 'ok',\n",
    "#             'report': f'보고서가 성공적으로 생성되었습니다. 다운로드 링크: {processedFilePath}',\n",
    "#             'processedFilePath': processedFilePath\n",
    "#         })\n",
    "#     else:\n",
    "#         return jsonify({\n",
    "#             'result': 'error',\n",
    "#             'error': '보고서 생성에 실패했습니다.'\n",
    "#         })\n",
    "# @app.route('/embedding', methods=['post'])\n",
    "# def new_calendar():\n",
    "#     params = request.json\n",
    "#     user_id = params.get('user_id')\n",
    "#     content = params.get('content')\n",
    "    \n",
    "#     if user_id == None or content == None:\n",
    "#         return jsonify({\n",
    "#             'result': 'fail'\n",
    "#         })\n",
    "    \n",
    "#     vs = PGVector(\n",
    "#         embeddings = embedding_model,\n",
    "#         collection_name = user_id,\n",
    "#         connection = connection,\n",
    "#         use_jsonb = True\n",
    "#     )\n",
    "    \n",
    "#     split_content = text_splitter.split_text(content)\n",
    "    \n",
    "#     print(split_content)\n",
    "    \n",
    "#     vs.add_texts(split_content)\n",
    "    \n",
    "#     return jsonify({\n",
    "#         'result': 'ok'\n",
    "#     }) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrok = ''\n",
    "ngrok = 'https://3482-34-125-254-167.ngrok-free.app'\n",
    "device = 'cpu'\n",
    "\n",
    "if ngrok == '':\n",
    "    llm_model = ChatOllama(\n",
    "        model='meta-llama-3.1',\n",
    "    )\n",
    "    llm_model_json = ChatOllama(\n",
    "        model='meta-llama-3.1',\n",
    "        format='json', \n",
    "    )\n",
    "else:\n",
    "    # Load language model\n",
    "    llm_model = ChatOllama(\n",
    "        model='meta-llama-3.1',\n",
    "        base_url=ngrok      # 주석 해제 시 코랩 자원으로 돌아감, # 주석 설정 시 로컬 자원으로 돌아감 \n",
    "    )\n",
    "    llm_model_json = ChatOllama(\n",
    "        model='meta-llama-3.1',\n",
    "        format='json', \n",
    "        base_url=ngrok       # 주석 해제 시 코랩 자원으로 돌아감, # 주석 설정 시 로컬 자원으로 돌아감 \n",
    "    )\n",
    "\n",
    "# Load embeddings model\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Define prompt\n",
    "prompt_template = '''Use the following pieces of context to answer the question at the end.\n",
    "If you don't find the answer in context, don't try to make up an answer.\n",
    "If you find the answer in context, answer me only use korean.\n",
    "\n",
    "context: {chat_history}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:'''\n",
    "rag_prompt = PromptTemplate(template=prompt_template, input_variables=['chat_history', 'question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = db_crawler('user_id')\n",
    "vectorstore = FAISS.from_texts(splits, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    "    return_messages=True\n",
    ")\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm_model,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    condense_question_prompt=rag_prompt,\n",
    "    memory=memory\n",
    ")\n",
    "# chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(\n",
    "    inputs={'human': '안녕하세요'},\n",
    "    outputs={'ai': '네, 무엇을 도와드릴까요.'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '9월 일정 알려줘',\n",
       " 'chat_history': [HumanMessage(content='9월 일정 알려줘'),\n",
       "  AIMessage(content='9월 일정은 다음과 같습니다.\\n9월 11일 12시부터 13시까지: 집에서 잠자기\\n9월 12일 16시부터 18시까지: 최종 발표 리허설!!\\n9월 13일 14시부터 16시까지: 최종 발표일')],\n",
       " 'answer': '9월 일정은 다음과 같습니다.\\n9월 11일 12시부터 13시까지: 집에서 잠자기\\n9월 12일 16시부터 18시까지: 최종 발표 리허설!!\\n9월 13일 14시부터 16시까지: 최종 발표일'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = '9월 일정 알려줘'\n",
    "res = conversation_chain({'question': question})\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '그 달 마지막 일정도 알려줘',\n",
       " 'chat_history': [HumanMessage(content='9월 일정 알려줘'),\n",
       "  AIMessage(content='9월의 일정은 다음과 같습니다.\\n9월 11일 12시부터 13시까지는 집에서 잠자기입니다.\\n9월 12일 16시부터 18시는 최종 발표 리허설입니다.\\n9월 13일 14시부터 16시까지는 최종 발표일입니다'),\n",
       "  HumanMessage(content='그 달 마지막 일정도 알려줘'),\n",
       "  AIMessage(content='9월의 마지막 일자는 30일입니다')],\n",
       " 'answer': '9월의 마지막 일자는 30일입니다'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = '그 달 마지막 일정도 알려줘'\n",
    "res = conversation_chain({'question': question})\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '그 날엔 아무 일정 없어?',\n",
       " 'chat_history': [HumanMessage(content='9월 일정 알려줘'),\n",
       "  AIMessage(content='9월의 일정은 다음과 같습니다.\\n9월 11일 12시부터 13시까지는 집에서 잠자기입니다.\\n9월 12일 16시부터 18시는 최종 발표 리허설입니다.\\n9월 13일 14시부터 16시까지는 최종 발표일입니다'),\n",
       "  HumanMessage(content='그 달 마지막 일정도 알려줘'),\n",
       "  AIMessage(content='9월의 마지막 일자는 30일입니다'),\n",
       "  HumanMessage(content='그 날엔 아무 일정 없어?'),\n",
       "  AIMessage(content='네, 9월 11일, 12일, 13일은 각각 집에서 잠자기, 최종 발표 리허설, 최종 발표일로 일정되어 있습니다. 하지만 9월 30일의 경우는 어떤 일정도 없습니다')],\n",
       " 'answer': '네, 9월 11일, 12일, 13일은 각각 집에서 잠자기, 최종 발표 리허설, 최종 발표일로 일정되어 있습니다. 하지만 9월 30일의 경우는 어떤 일정도 없습니다'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = '그 날엔 아무 일정 없어?'\n",
    "res = conversation_chain({'question': question})\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'오늘은 2024-09-04, Wednesday입니다.'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'오늘은 2024-09-04, Wednesday입니다'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'오늘은 {today}입니다'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
