{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_MkEgBwrWm2"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "hf_hub_download(repo_id=\"lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF\", filename=\"Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\", local_dir='/ollama')\n",
        "# hf_hub_download(repo_id=\"lmstudio-community/Meta-Llama-3.1-70B-Instruct-GGUF\", filename=\"Meta-Llama-3.1-70B-Instruct-Q4_K_M.gguf\", local_dir='/ollama')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "084WqtUCOYFd"
      },
      "outputs": [],
      "source": [
        "with open('/ollama/Modelfile', 'w') as f:\n",
        "  f.write('''FROM Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\n",
        "\n",
        "TEMPLATE \"\"\"{{- if .System }}\n",
        "<s>{{ .System }}</s>\n",
        "{{- end }}\n",
        "<s>Human:\n",
        "{{ .Prompt }}</s>\n",
        "<s>Assistant:\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"\"\"\n",
        "\n",
        "PARAMETER temperature 0\n",
        "PARAMETER num_predict 3000\n",
        "PARAMETER num_ctx 4096\n",
        "PARAMETER stop <s>\n",
        "PARAMETER stop </s>''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zVMHOLBA_hZK"
      },
      "outputs": [],
      "source": [
        "!curl https://ollama.ai/install.sh | sh\n",
        "\n",
        "!echo 'debconf debconf/frontend select Noninteractive' | sudo debconf-set-selections\n",
        "!sudo apt-get update && sudo apt-get install -y cuda-drivers\n",
        "\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSAHvNfMKba_"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "token=\"2kXCJ8cNTZQ1yaMnUKLvMPGhr7o_5SAbcqqvBv2vThLiWrbhY\"\n",
        "\n",
        "ngrok.set_auth_token(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hsOxUtyxyZi"
      },
      "outputs": [],
      "source": [
        "%cd /ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USsJS0U-LilJ",
        "outputId": "6bf4a22e-af4a-4872-d330-081c53d1ccef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> starting ollama serve\n",
            ">>> starting ngrok http --log stderr 11434 --host-header=\"localhost:11434\"\n",
            ">>> starting ollama create meta-llama-3.1\n",
            "Couldn't find '/root/.ollama/id_ed25519'. Generating new private key.\n",
            "Your new public key is:\n",
            "\n",
            "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIAwat/xico3lyghDtftQwEgyfCXyjWULgApyLntzT8hD\n",
            "\n",
            "2024/09/09 00:48:45 routes.go:1125: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]\"\n",
            "time=2024-09-09T00:48:45.418Z level=INFO source=images.go:753 msg=\"total blobs: 0\"\n",
            "time=2024-09-09T00:48:45.418Z level=INFO source=images.go:760 msg=\"total unused blobs removed: 0\"\n",
            "time=2024-09-09T00:48:45.418Z level=INFO source=routes.go:1172 msg=\"Listening on 127.0.0.1:11434 (version 0.3.9)\"\n",
            "time=2024-09-09T00:48:45.419Z level=INFO source=payload.go:30 msg=\"extracting embedded files\" dir=/tmp/ollama1720021251/runners\n",
            "t=2024-09-09T00:48:45+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "t=2024-09-09T00:48:45+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.config/ngrok/ngrok.yml\n",
            "t=2024-09-09T00:48:45+0000 lvl=info msg=\"open config file\" path=/root/.config/ngrok/ngrok.yml err=nil\n",
            "t=2024-09-09T00:48:45+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
            "t=2024-09-09T00:48:46+0000 lvl=info msg=\"client session established\" obj=tunnels.session\n",
            "t=2024-09-09T00:48:46+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "t=2024-09-09T00:48:46+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=command_line addr=http://localhost:11434 url=https://b9b3-34-142-171-135.ngrok-free.app\n",
            "time=2024-09-09T00:49:18.054Z level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [cuda_v11 cuda_v12 rocm_v60102 cpu cpu_avx cpu_avx2]\"\n",
            "time=2024-09-09T00:49:18.054Z level=INFO source=gpu.go:200 msg=\"looking for compatible GPUs\"\n",
            "time=2024-09-09T00:49:18.392Z level=INFO source=types.go:107 msg=\"inference compute\" id=GPU-8c02477b-6e9a-2a90-9216-fd0d3d476c66 library=cuda variant=v12 compute=7.5 driver=12.2 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.6 GiB\"\n",
            "[GIN] 2024/09/09 - 00:49:18 | 200 |      43.666µs |       127.0.0.1 | HEAD     \"/\"\n",
            "t=2024-09-09T00:49:55+0000 lvl=info msg=\"join connections\" obj=join id=34e1185a53cb l=127.0.0.1:11434 r=27.0.238.70:40350\n",
            "[GIN] 2024/09/09 - 00:49:55 | 200 |      36.874µs |     27.0.238.70 | GET      \"/\"\n",
            "[GIN] 2024/09/09 - 00:50:36 | 201 | 49.349354632s |       127.0.0.1 | POST     \"/api/blobs/sha256:f2be3e1a239c12c9f3f01a962b11fb2807f8032fdb63b0a5502ea42ddef55e44\"\n",
            "[GIN] 2024/09/09 - 00:50:36 | 200 |  256.511683ms |       127.0.0.1 | POST     \"/api/create\"\n",
            "\u001b[?25ltransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 0% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 0% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 0% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 0% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 1% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 1% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 1% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 1% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 1% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 2% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 2% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 2% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 2% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 2% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 2% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 3% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 3% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 3% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 3% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 3% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 3% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 4% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 4% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 4% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 4% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 4% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 4% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 5% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 5% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 5% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 5% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 6% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 6% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 6% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 6% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 6% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 6% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 7% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 7% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 7% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 7% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 7% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 7% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 8% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 8% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 8% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 8% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 8% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 8% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 9% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 9% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 9% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 9% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 10% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 10% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 10% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 10% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 10% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 10% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 11% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 11% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 11% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 11% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 11% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 13% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 13% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 13% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 13% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 14% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 14% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 14% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 15% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 15% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 15% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 15% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 16% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 16% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 16% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 17% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 17% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 17% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 18% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 18% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 18% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 19% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 19% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 19% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 20% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 20% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 20% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 21% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 21% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 21% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 21% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 22% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 22% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 23% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 23% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 23% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 23% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 24% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 24% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 24% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 26% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 26% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 26% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 27% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 27% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 27% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 27% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 27% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 27% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 28% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 28% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 28% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 28% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 29% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 29% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 29% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 30% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 30% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 30% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 31% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 31% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 31% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 31% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 31% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 31% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 32% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 32% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 33% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 33% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 34% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 34% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 34% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 34% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 34% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 34% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 35% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 35% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 35% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 35% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 35% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 36% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 36% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 36% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 36% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 36% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 38% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 38% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 38% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 38% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 38% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 38% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 39% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 39% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 39% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 39% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 39% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 40% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 40% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 40% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 40% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 40% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 41% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 41% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 41% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 41% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 41% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 42% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 42% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 42% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 42% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 42% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 42% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 42% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 43% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 43% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 43% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 43% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 43% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 43% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 44% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 44% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 44% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 44% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 45% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 45% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 45% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 45% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 45% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 45% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 45% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 46% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 46% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 46% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 46% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 46% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 47% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 47% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 47% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 47% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 47% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 49% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 49% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 49% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 49% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 49% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 49% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 50% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 50% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 50% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 50% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 50% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 51% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 51% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 51% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 51% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 51% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 51% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 52% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 52% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 52% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 52% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 52% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 53% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 53% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 53% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 53% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 55% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 55% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 55% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 55% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 55% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 55% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 55% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 56% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 56% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 56% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 56% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 56% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 57% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 57% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 57% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 57% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 57% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 57% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 58% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 58% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 58% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 58% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 58% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 59% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 59% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 59% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 59% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 59% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 59% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 59% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 60% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 60% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 60% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 60% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 60% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 61% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 61% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 61% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 61% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 61% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 62% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 62% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 62% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 62% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 63% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 63% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 63% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 63% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 63% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 63% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 64% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 64% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 64% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 64% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 64% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 64% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 66% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 66% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 66% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 66% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 66% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 68% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 68% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 68% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 68% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 68% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 68% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 69% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 69% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 69% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 69% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 69% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 70% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 70% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 70% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 70% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 71% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 71% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 71% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 71% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 71% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 71% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 72% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 72% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 72% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 72% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 72% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 73% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 73% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 73% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 73% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 73% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 74% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 74% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 74% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 74% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 74% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 74% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 74% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 75% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 75% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 75% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 75% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 76% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 76% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 76% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 76% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 76% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 77% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 77% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 77% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 78% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 78% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 78% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 78% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 79% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 79% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 79% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 81% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 81% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 81% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 82% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 82% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 82% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 83% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 83% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 83% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 84% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 84% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 84% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 84% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 85% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 85% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 85% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 86% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 86% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 86% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 87% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 87% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 87% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 88% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 88% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 88% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 89% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 89% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 89% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 89% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 90% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 90% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 90% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 91% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 91% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 91% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 92% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 92% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 92% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 93% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 93% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 93% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 93% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 94% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 94% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 94% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 95% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 95% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 96% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 96% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 96% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 96% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 98% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 98% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 98% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 99% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 99% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 99% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 100% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 100% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 100% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 100%\n",
            "using existing layer sha256:f2be3e1a239c12c9f3f01a962b11fb2807f8032fdb63b0a5502ea42ddef55e44\n",
            "creating new layer sha256:6b70a2ad0d545ca50d11b293ba6f6355eff16363425c8b163289014cf19311fc\n",
            "creating new layer sha256:1fa69e2371b762d1882b0bd98d284f312a36c27add732016e12e52586f98a9f5\n",
            "creating new layer sha256:0fefad39836f0b7c8af0eb6d8917daada1e7bc1855bb0dd2c095d6f56b127f16\n",
            "creating new layer sha256:04dd1370de81a30a7ddc9290ddc7fd9035b80196806fbc2237d32189516983bf\n",
            "writing manifest\n",
            "success \u001b[?25h\n",
            "t=2024-09-09T00:54:07+0000 lvl=info msg=\"join connections\" obj=join id=64584e3b7447 l=127.0.0.1:11434 r=125.244.144.224:49882\n",
            "time=2024-09-09T00:54:07.881Z level=INFO source=sched.go:715 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-f2be3e1a239c12c9f3f01a962b11fb2807f8032fdb63b0a5502ea42ddef55e44 gpu=GPU-8c02477b-6e9a-2a90-9216-fd0d3d476c66 parallel=4 available=15727656960 required=\"8.0 GiB\"\n",
            "time=2024-09-09T00:54:07.883Z level=INFO source=memory.go:309 msg=\"offload to cuda\" layers.requested=-1 layers.model=33 layers.offload=33 layers.split=\"\" memory.available=\"[14.6 GiB]\" memory.required.full=\"8.0 GiB\" memory.required.partial=\"8.0 GiB\" memory.required.kv=\"2.0 GiB\" memory.required.allocations=\"[8.0 GiB]\" memory.weights.total=\"5.9 GiB\" memory.weights.repeating=\"5.5 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"1.1 GiB\" memory.graph.partial=\"1.1 GiB\"\n",
            "time=2024-09-09T00:54:07.885Z level=INFO source=server.go:391 msg=\"starting llama server\" cmd=\"/tmp/ollama1720021251/runners/cuda_v12/ollama_llama_server --model /root/.ollama/models/blobs/sha256-f2be3e1a239c12c9f3f01a962b11fb2807f8032fdb63b0a5502ea42ddef55e44 --ctx-size 16384 --batch-size 512 --embedding --log-disable --n-gpu-layers 33 --parallel 4 --port 40765\"\n",
            "time=2024-09-09T00:54:07.887Z level=INFO source=sched.go:450 msg=\"loaded runners\" count=1\n",
            "time=2024-09-09T00:54:07.887Z level=INFO source=server.go:591 msg=\"waiting for llama runner to start responding\"\n",
            "time=2024-09-09T00:54:07.888Z level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server error\"\n",
            "INFO [main] build info | build=1 commit=\"1e6f655\" tid=\"139131465273344\" timestamp=1725843248\n",
            "INFO [main] system info | n_threads=1 n_threads_batch=-1 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"139131465273344\" timestamp=1725843248 total_threads=2\n",
            "INFO [main] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"6\" port=\"40765\" tid=\"139131465273344\" timestamp=1725843248\n",
            "llama_model_loader: loaded meta data with 33 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-f2be3e1a239c12c9f3f01a962b11fb2807f8032fdb63b0a5502ea42ddef55e44 (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                            general.license str              = llama3.1\n",
            "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
            "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
            "llama_model_loader: - kv   9:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "time=2024-09-09T00:54:08.641Z level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Meta-Llama-3.1-8B-Instruc...\n",
            "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 224\n",
            "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW)\n",
            "llm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   281.81 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  4403.50 MiB\n",
            "llama_new_context_with_model: n_ctx      = 16384\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 500000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     2.02 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =  1088.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    40.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "INFO [main] model loaded | tid=\"139131465273344\" timestamp=1725843272\n",
            "time=2024-09-09T00:54:32.992Z level=INFO source=server.go:630 msg=\"llama runner started in 25.10 seconds\"\n",
            "[GIN] 2024/09/09 - 00:54:38 | 200 | 31.320619162s | 125.244.144.224 | POST     \"/api/chat\"\n",
            "t=2024-09-09T00:54:40+0000 lvl=info msg=\"join connections\" obj=join id=2a6ea025ce5a l=127.0.0.1:11434 r=125.244.144.224:49887\n",
            "[GIN] 2024/09/09 - 00:54:44 | 200 |  4.246008942s | 125.244.144.224 | POST     \"/api/chat\"\n",
            "t=2024-09-09T00:54:45+0000 lvl=info msg=\"join connections\" obj=join id=80a8736a9e72 l=127.0.0.1:11434 r=125.244.144.224:49888\n",
            "[GIN] 2024/09/09 - 00:54:52 | 200 |  6.836445502s | 125.244.144.224 | POST     \"/api/chat\"\n",
            "t=2024-09-09T00:54:53+0000 lvl=info msg=\"join connections\" obj=join id=6038bc0995b1 l=127.0.0.1:11434 r=125.244.144.224:49889\n",
            "[GIN] 2024/09/09 - 00:54:58 | 200 |  4.167606214s | 125.244.144.224 | POST     \"/api/chat\"\n",
            "t=2024-09-09T01:02:16+0000 lvl=info msg=\"join connections\" obj=join id=3502a143fb72 l=127.0.0.1:11434 r=125.244.144.224:49925\n",
            "time=2024-09-09T01:02:16.600Z level=INFO source=sched.go:715 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-f2be3e1a239c12c9f3f01a962b11fb2807f8032fdb63b0a5502ea42ddef55e44 gpu=GPU-8c02477b-6e9a-2a90-9216-fd0d3d476c66 parallel=4 available=15727656960 required=\"8.0 GiB\"\n",
            "time=2024-09-09T01:02:16.602Z level=INFO source=memory.go:309 msg=\"offload to cuda\" layers.requested=-1 layers.model=33 layers.offload=33 layers.split=\"\" memory.available=\"[14.6 GiB]\" memory.required.full=\"8.0 GiB\" memory.required.partial=\"8.0 GiB\" memory.required.kv=\"2.0 GiB\" memory.required.allocations=\"[8.0 GiB]\" memory.weights.total=\"5.9 GiB\" memory.weights.repeating=\"5.5 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"1.1 GiB\" memory.graph.partial=\"1.1 GiB\"\n",
            "time=2024-09-09T01:02:16.604Z level=INFO source=server.go:391 msg=\"starting llama server\" cmd=\"/tmp/ollama1720021251/runners/cuda_v12/ollama_llama_server --model /root/.ollama/models/blobs/sha256-f2be3e1a239c12c9f3f01a962b11fb2807f8032fdb63b0a5502ea42ddef55e44 --ctx-size 16384 --batch-size 512 --embedding --log-disable --n-gpu-layers 33 --parallel 4 --port 39193\"\n",
            "time=2024-09-09T01:02:16.605Z level=INFO source=sched.go:450 msg=\"loaded runners\" count=1\n",
            "time=2024-09-09T01:02:16.605Z level=INFO source=server.go:591 msg=\"waiting for llama runner to start responding\"\n",
            "time=2024-09-09T01:02:16.606Z level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server error\"\n",
            "INFO [main] build info | build=1 commit=\"1e6f655\" tid=\"136482959237120\" timestamp=1725843736\n",
            "INFO [main] system info | n_threads=1 n_threads_batch=-1 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"136482959237120\" timestamp=1725843736 total_threads=2\n",
            "INFO [main] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"6\" port=\"39193\" tid=\"136482959237120\" timestamp=1725843736\n",
            "llama_model_loader: loaded meta data with 33 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-f2be3e1a239c12c9f3f01a962b11fb2807f8032fdb63b0a5502ea42ddef55e44 (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                            general.license str              = llama3.1\n",
            "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
            "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
            "llama_model_loader: - kv   9:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "time=2024-09-09T01:02:17.107Z level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Meta-Llama-3.1-8B-Instruc...\n",
            "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 224\n",
            "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW)\n",
            "llm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   281.81 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  4403.50 MiB\n",
            "llama_new_context_with_model: n_ctx      = 16384\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 500000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     2.02 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =  1088.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    40.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "INFO [main] model loaded | tid=\"136482959237120\" timestamp=1725843740\n",
            "time=2024-09-09T01:02:20.621Z level=INFO source=server.go:630 msg=\"llama runner started in 4.02 seconds\"\n",
            "[GIN] 2024/09/09 - 01:02:28 | 200 |  12.21089312s | 125.244.144.224 | POST     \"/api/chat\"\n",
            "t=2024-09-09T01:02:29+0000 lvl=info msg=\"join connections\" obj=join id=a4c5ece912e6 l=127.0.0.1:11434 r=125.244.144.224:49927\n",
            "[GIN] 2024/09/09 - 01:02:31 | 200 |  1.309465051s | 125.244.144.224 | POST     \"/api/chat\"\n",
            "t=2024-09-09T01:02:32+0000 lvl=info msg=\"join connections\" obj=join id=a173c4ce2458 l=127.0.0.1:11434 r=125.244.144.224:49928\n",
            "[GIN] 2024/09/09 - 01:02:33 | 200 |   1.12263353s | 125.244.144.224 | POST     \"/api/chat\"\n",
            "t=2024-09-09T01:02:34+0000 lvl=info msg=\"join connections\" obj=join id=009fd7b22060 l=127.0.0.1:11434 r=125.244.144.224:49929\n",
            "[GIN] 2024/09/09 - 01:02:36 | 200 |  1.540759102s | 125.244.144.224 | POST     \"/api/chat\"\n",
            "t=2024-09-09T01:02:37+0000 lvl=info msg=\"join connections\" obj=join id=167b227ad6da l=127.0.0.1:11434 r=125.244.144.224:49941\n",
            "[GIN] 2024/09/09 - 01:02:39 | 200 |  1.504188288s | 125.244.144.224 | POST     \"/api/chat\"\n",
            "t=2024-09-09T01:04:17+0000 lvl=info msg=\"join connections\" obj=join id=e85bc6d3dc7f l=127.0.0.1:11434 r=125.244.144.224:49959\n",
            "[GIN] 2024/09/09 - 01:04:24 | 200 |  7.502724089s | 125.244.144.224 | POST     \"/api/chat\"\n",
            "t=2024-09-09T01:04:26+0000 lvl=info msg=\"join connections\" obj=join id=080b0568a876 l=127.0.0.1:11434 r=125.244.144.224:49960\n",
            "[GIN] 2024/09/09 - 01:04:27 | 200 |  1.324031233s | 125.244.144.224 | POST     \"/api/chat\"\n",
            "t=2024-09-09T01:04:28+0000 lvl=info msg=\"join connections\" obj=join id=a90bd6dc8e55 l=127.0.0.1:11434 r=125.244.144.224:49962\n",
            "[GIN] 2024/09/09 - 01:04:30 | 200 |  1.081840661s | 125.244.144.224 | POST     \"/api/chat\"\n",
            "t=2024-09-09T01:04:32+0000 lvl=info msg=\"join connections\" obj=join id=e127fc74d651 l=127.0.0.1:11434 r=125.244.144.224:49963\n",
            "[GIN] 2024/09/09 - 01:04:33 | 200 |  1.382805777s | 125.244.144.224 | POST     \"/api/chat\"\n",
            "t=2024-09-09T01:04:34+0000 lvl=info msg=\"join connections\" obj=join id=8cfdaf485364 l=127.0.0.1:11434 r=125.244.144.224:49964\n",
            "[GIN] 2024/09/09 - 01:04:35 | 200 |  1.272588577s | 125.244.144.224 | POST     \"/api/chat\"\n",
            "t=2024-09-09T01:21:17+0000 lvl=info msg=\"join connections\" obj=join id=b2a80dad3de7 l=127.0.0.1:11434 r=125.244.144.224:50066\n",
            "time=2024-09-09T01:21:17.656Z level=INFO source=sched.go:715 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-f2be3e1a239c12c9f3f01a962b11fb2807f8032fdb63b0a5502ea42ddef55e44 gpu=GPU-8c02477b-6e9a-2a90-9216-fd0d3d476c66 parallel=4 available=15727656960 required=\"8.0 GiB\"\n",
            "time=2024-09-09T01:21:17.658Z level=INFO source=memory.go:309 msg=\"offload to cuda\" layers.requested=-1 layers.model=33 layers.offload=33 layers.split=\"\" memory.available=\"[14.6 GiB]\" memory.required.full=\"8.0 GiB\" memory.required.partial=\"8.0 GiB\" memory.required.kv=\"2.0 GiB\" memory.required.allocations=\"[8.0 GiB]\" memory.weights.total=\"5.9 GiB\" memory.weights.repeating=\"5.5 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"1.1 GiB\" memory.graph.partial=\"1.1 GiB\"\n",
            "time=2024-09-09T01:21:17.659Z level=INFO source=server.go:391 msg=\"starting llama server\" cmd=\"/tmp/ollama1720021251/runners/cuda_v12/ollama_llama_server --model /root/.ollama/models/blobs/sha256-f2be3e1a239c12c9f3f01a962b11fb2807f8032fdb63b0a5502ea42ddef55e44 --ctx-size 16384 --batch-size 512 --embedding --log-disable --n-gpu-layers 33 --parallel 4 --port 41177\"\n",
            "time=2024-09-09T01:21:17.660Z level=INFO source=sched.go:450 msg=\"loaded runners\" count=1\n",
            "time=2024-09-09T01:21:17.660Z level=INFO source=server.go:591 msg=\"waiting for llama runner to start responding\"\n",
            "time=2024-09-09T01:21:17.660Z level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server error\"\n",
            "INFO [main] build info | build=1 commit=\"1e6f655\" tid=\"133391803383808\" timestamp=1725844877\n",
            "INFO [main] system info | n_threads=1 n_threads_batch=-1 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"133391803383808\" timestamp=1725844877 total_threads=2\n",
            "INFO [main] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"6\" port=\"41177\" tid=\"133391803383808\" timestamp=1725844877\n",
            "llama_model_loader: loaded meta data with 33 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-f2be3e1a239c12c9f3f01a962b11fb2807f8032fdb63b0a5502ea42ddef55e44 (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                            general.license str              = llama3.1\n",
            "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
            "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
            "llama_model_loader: - kv   9:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "time=2024-09-09T01:21:17.911Z level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Meta-Llama-3.1-8B-Instruc...\n",
            "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 224\n",
            "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW)\n",
            "llm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
            "time=2024-09-09T01:21:19.367Z level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server not responding\"\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   281.81 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  4403.50 MiB\n",
            "time=2024-09-09T01:21:19.619Z level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
            "llama_new_context_with_model: n_ctx      = 16384\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 500000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     2.02 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =  1088.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    40.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "INFO [main] model loaded | tid=\"133391803383808\" timestamp=1725844881\n",
            "time=2024-09-09T01:21:21.242Z level=INFO source=server.go:630 msg=\"llama runner started in 3.58 seconds\"\n",
            "[GIN] 2024/09/09 - 01:21:22 | 200 |  5.013191233s | 125.244.144.224 | POST     \"/api/chat\"\n",
            "t=2024-09-09T01:28:57+0000 lvl=info msg=\"join connections\" obj=join id=11addad8b69d l=127.0.0.1:11434 r=125.244.144.224:50216\n",
            "time=2024-09-09T01:28:57.723Z level=INFO source=sched.go:715 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-f2be3e1a239c12c9f3f01a962b11fb2807f8032fdb63b0a5502ea42ddef55e44 gpu=GPU-8c02477b-6e9a-2a90-9216-fd0d3d476c66 parallel=4 available=15727656960 required=\"8.0 GiB\"\n",
            "time=2024-09-09T01:28:57.725Z level=INFO source=memory.go:309 msg=\"offload to cuda\" layers.requested=-1 layers.model=33 layers.offload=33 layers.split=\"\" memory.available=\"[14.6 GiB]\" memory.required.full=\"8.0 GiB\" memory.required.partial=\"8.0 GiB\" memory.required.kv=\"2.0 GiB\" memory.required.allocations=\"[8.0 GiB]\" memory.weights.total=\"5.9 GiB\" memory.weights.repeating=\"5.5 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"1.1 GiB\" memory.graph.partial=\"1.1 GiB\"\n",
            "time=2024-09-09T01:28:57.725Z level=INFO source=server.go:391 msg=\"starting llama server\" cmd=\"/tmp/ollama1720021251/runners/cuda_v12/ollama_llama_server --model /root/.ollama/models/blobs/sha256-f2be3e1a239c12c9f3f01a962b11fb2807f8032fdb63b0a5502ea42ddef55e44 --ctx-size 16384 --batch-size 512 --embedding --log-disable --n-gpu-layers 33 --parallel 4 --port 46227\"\n",
            "time=2024-09-09T01:28:57.726Z level=INFO source=sched.go:450 msg=\"loaded runners\" count=1\n",
            "time=2024-09-09T01:28:57.726Z level=INFO source=server.go:591 msg=\"waiting for llama runner to start responding\"\n",
            "time=2024-09-09T01:28:57.727Z level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server error\"\n",
            "INFO [main] build info | build=1 commit=\"1e6f655\" tid=\"137368548843520\" timestamp=1725845337\n",
            "INFO [main] system info | n_threads=1 n_threads_batch=-1 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"137368548843520\" timestamp=1725845337 total_threads=2\n",
            "INFO [main] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"6\" port=\"46227\" tid=\"137368548843520\" timestamp=1725845337\n",
            "llama_model_loader: loaded meta data with 33 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-f2be3e1a239c12c9f3f01a962b11fb2807f8032fdb63b0a5502ea42ddef55e44 (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                            general.license str              = llama3.1\n",
            "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
            "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
            "llama_model_loader: - kv   9:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Meta-Llama-3.1-8B-Instruc...\n",
            "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 224\n",
            "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "time=2024-09-09T01:28:57.978Z level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW)\n",
            "llm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   281.81 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  4403.50 MiB\n",
            "llama_new_context_with_model: n_ctx      = 16384\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 500000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     2.02 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =  1088.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    40.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "INFO [main] model loaded | tid=\"137368548843520\" timestamp=1725845340\n",
            "time=2024-09-09T01:29:00.489Z level=INFO source=server.go:630 msg=\"llama runner started in 2.76 seconds\"\n",
            "[GIN] 2024/09/09 - 01:29:02 | 200 |  4.509191538s | 125.244.144.224 | POST     \"/api/chat\"\n",
            "t=2024-09-09T01:29:03+0000 lvl=info msg=\"join connections\" obj=join id=45cbdef2eacf l=127.0.0.1:11434 r=125.244.144.224:50217\n",
            "[GIN] 2024/09/09 - 01:29:05 | 200 |  1.916393395s | 125.244.144.224 | POST     \"/api/chat\"\n",
            "t=2024-09-09T01:29:43+0000 lvl=info msg=\"join connections\" obj=join id=c50c373ae3f2 l=127.0.0.1:11434 r=125.244.144.224:50230\n",
            "[GIN] 2024/09/09 - 01:29:44 | 200 |  1.065898233s | 125.244.144.224 | POST     \"/api/chat\"\n",
            "t=2024-09-09T01:29:45+0000 lvl=info msg=\"join connections\" obj=join id=877129ce74dd l=127.0.0.1:11434 r=125.244.144.224:50231\n",
            "[GIN] 2024/09/09 - 01:29:47 | 200 |  1.292178507s | 125.244.144.224 | POST     \"/api/chat\"\n",
            "t=2024-09-09T01:54:52+0000 lvl=info msg=\"join connections\" obj=join id=86e37d294ab3 l=127.0.0.1:11434 r=211.231.103.93:43040\n",
            "[GIN] 2024/09/09 - 01:54:52 | 200 |      66.399µs |  211.231.103.93 | GET      \"/\"\n",
            "t=2024-09-09T02:08:21+0000 lvl=info msg=\"join connections\" obj=join id=ead6e5c57baa l=127.0.0.1:11434 r=125.244.144.224:51240\n",
            "time=2024-09-09T02:08:21.897Z level=INFO source=sched.go:715 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-f2be3e1a239c12c9f3f01a962b11fb2807f8032fdb63b0a5502ea42ddef55e44 gpu=GPU-8c02477b-6e9a-2a90-9216-fd0d3d476c66 parallel=4 available=15727656960 required=\"8.0 GiB\"\n",
            "time=2024-09-09T02:08:21.898Z level=INFO source=memory.go:309 msg=\"offload to cuda\" layers.requested=-1 layers.model=33 layers.offload=33 layers.split=\"\" memory.available=\"[14.6 GiB]\" memory.required.full=\"8.0 GiB\" memory.required.partial=\"8.0 GiB\" memory.required.kv=\"2.0 GiB\" memory.required.allocations=\"[8.0 GiB]\" memory.weights.total=\"5.9 GiB\" memory.weights.repeating=\"5.5 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"1.1 GiB\" memory.graph.partial=\"1.1 GiB\"\n",
            "time=2024-09-09T02:08:21.899Z level=INFO source=server.go:391 msg=\"starting llama server\" cmd=\"/tmp/ollama1720021251/runners/cuda_v12/ollama_llama_server --model /root/.ollama/models/blobs/sha256-f2be3e1a239c12c9f3f01a962b11fb2807f8032fdb63b0a5502ea42ddef55e44 --ctx-size 16384 --batch-size 512 --embedding --log-disable --n-gpu-layers 33 --parallel 4 --port 36839\"\n",
            "time=2024-09-09T02:08:21.899Z level=INFO source=sched.go:450 msg=\"loaded runners\" count=1\n",
            "time=2024-09-09T02:08:21.900Z level=INFO source=server.go:591 msg=\"waiting for llama runner to start responding\"\n",
            "time=2024-09-09T02:08:21.900Z level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server error\"\n",
            "INFO [main] build info | build=1 commit=\"1e6f655\" tid=\"132738112618496\" timestamp=1725847701\n",
            "INFO [main] system info | n_threads=1 n_threads_batch=-1 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"132738112618496\" timestamp=1725847701 total_threads=2\n",
            "INFO [main] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"6\" port=\"36839\" tid=\"132738112618496\" timestamp=1725847701\n",
            "llama_model_loader: loaded meta data with 33 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-f2be3e1a239c12c9f3f01a962b11fb2807f8032fdb63b0a5502ea42ddef55e44 (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                            general.license str              = llama3.1\n",
            "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
            "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
            "llama_model_loader: - kv   9:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Meta-Llama-3.1-8B-Instruc...\n",
            "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 224\n",
            "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "time=2024-09-09T02:08:22.152Z level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW)\n",
            "llm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   281.81 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  4403.50 MiB\n",
            "llama_new_context_with_model: n_ctx      = 16384\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 500000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     2.02 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =  1088.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    40.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "INFO [main] model loaded | tid=\"132738112618496\" timestamp=1725847704\n",
            "time=2024-09-09T02:08:24.663Z level=INFO source=server.go:630 msg=\"llama runner started in 2.76 seconds\"\n",
            "[GIN] 2024/09/09 - 02:08:25 | 200 |  4.063235632s | 125.244.144.224 | POST     \"/api/chat\"\n",
            "t=2024-09-09T04:16:47+0000 lvl=info msg=\"join connections\" obj=join id=65b39b38ea4c l=127.0.0.1:11434 r=211.231.103.91:46064\n",
            "[GIN] 2024/09/09 - 04:16:47 | 200 |      46.498µs |  211.231.103.91 | GET      \"/\"\n",
            "t=2024-09-09T06:10:56+0000 lvl=info msg=\"join connections\" obj=join id=7332389bb8d4 l=127.0.0.1:11434 r=211.231.103.106:46074\n",
            "[GIN] 2024/09/09 - 06:10:56 | 200 |      48.067µs | 211.231.103.106 | GET      \"/\"\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import asyncio\n",
        "\n",
        "# Set LD_LIBRARY_PATH so the system NVIDIA library\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n",
        "\n",
        "async def run_process(cmd):\n",
        "  print('>>> starting', *cmd)\n",
        "  p = await asyncio.subprocess.create_subprocess_exec(\n",
        "      *cmd,\n",
        "      stdout=asyncio.subprocess.PIPE,\n",
        "      stderr=asyncio.subprocess.PIPE,\n",
        "      limit=1024 *512\n",
        "  )\n",
        "\n",
        "  async def pipe(lines):\n",
        "    # print(lines)\n",
        "    async for line in lines:\n",
        "      print(line.strip().decode('utf-8'))\n",
        "\n",
        "  await asyncio.gather(\n",
        "      pipe(p.stdout),\n",
        "      pipe(p.stderr),\n",
        "  )\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "\n",
        "\n",
        "await asyncio.gather(\n",
        "  run_process(['ollama', 'serve']),\n",
        "  run_process(['ngrok', 'http', '--log', 'stderr', '11434', '--host-header=\"localhost:11434\"']),\n",
        "  run_process(['ollama', 'create', 'meta-llama-3.1'])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuXtyYaXcHWB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
