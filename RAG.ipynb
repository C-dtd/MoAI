{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.ncloud-forums.com/topic/277 <br>\n",
    "https://velog.io/@kwon0koang/%EB%A1%9C%EC%BB%AC%EC%97%90%EC%84%9C-Llama3-%EB%8F%8C%EB%A6%AC%EA%B8%B0#-rag <br>\n",
    "RAG에 대한 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "strftime() missing required argument 'format' (pos 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mtoday()\u001b[38;5;241m.\u001b[39mstrftime()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: strftime() missing required argument 'format' (pos 1)"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "f'{datetime.datetime.today().strftime('')}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs = {'device': 'cpu'}, # 모델이 CPU에서 실행되도록 설정. GPU를 사용할 수 있는 환경이라면 'cuda'로 설정할 수도 있음\n",
    "    encode_kwargs = {'normalize_embeddings': True}, # 임베딩 정규화. 모든 벡터가 같은 범위의 값을 갖도록 함. 유사도 계산 시 일관성을 높여줌\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "doclist = [\n",
    "    './문서/potato.txt',\n",
    "    './문서/sweetpotato.txt',\n",
    "]\n",
    "data = []\n",
    "\n",
    "for doc in doclist:\n",
    "    loader = TextLoader(doc)\n",
    "    docdata = loader.load()\n",
    "    data += docdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://wikidocs.net/234014 <br>\n",
    "FAISS에 대한 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    splits,\n",
    "    embedding = embedding_model,\n",
    ")\n",
    "\n",
    "MY_FAISS_INDEX = \"MY_FAISS_INDEX\"\n",
    "vectorstore.save_local(MY_FAISS_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "MY_FAISS_INDEX = \"MY_FAISS_INDEX\"\n",
    "\n",
    "vectorstore = FAISS.load_local(\n",
    "    MY_FAISS_INDEX, \n",
    "    embedding_model, \n",
    "    allow_dangerous_deserialization=True #잠재적으로 위험한 데이터 구조나 객체를 포함할 수 있는 인덱스 파일의 로딩을 허용\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10}) # 유사도 높은 10문장 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='가족: 가지과 (Solanaceae)\\n형태: 감자는 지하에 있는 덩이줄기(혹은 괴경)가 식용 부분입니다. 이 덩이줄기는 영양분을 저장하는 역할을 합니다.', metadata={'source': './문서/potato.txt'}),\n",
       " Document(page_content='건강 효과:\\n에너지 공급: 감자는 탄수화물이 풍부해 에너지 공급원으로 좋습니다.\\n소화 개선: 식이섬유가 풍부해 소화 건강에 도움이 됩니다.', metadata={'source': './문서/potato.txt'}),\n",
       " Document(page_content='4. 영양 성분 및 건강 효과\\n영양 성분: 감자는 비타민 C, 비타민 B6, 칼륨, 철분, 식이섬유 등이 풍부합니다. 감자는 칼로리가 낮고, 지방이 거의 없습니다.\\n건강 효과:', metadata={'source': './문서/potato.txt'}),\n",
       " Document(page_content='심장 건강: 칼륨은 혈압 조절에 도움이 되어 심장 건강에 좋습니다.\\n항산화 효과: 비타민 C와 페놀 화합물 등이 항산화 효과를 제공합니다.\\n5. 요리 및 활용', metadata={'source': './문서/potato.txt'}),\n",
       " Document(page_content='감자는 전 세계적으로 중요한 식량 자원 중 하나로, 주로 식용으로 사용되는 식물입니다. 다음은 감자에 대한 자세한 설명입니다:\\n1. 역사 및 기원', metadata={'source': './문서/potato.txt'}),\n",
       " Document(page_content='6. 기타 정보\\n보관: 감자는 서늘하고 어두운 곳에서 보관해야 합니다. 빛에 노출되면 녹색으로 변하고 솔라닌이라는 독성 물질이 생길 수 있습니다.', metadata={'source': './문서/potato.txt'}),\n",
       " Document(page_content='5. 요리 및 활용\\n요리법: 감자는 다양하게 요리될 수 있습니다. 감자튀김, 매시드 포테이토, 감자 샐러드, 구운 감자 등 여러 가지 요리법이 있습니다.', metadata={'source': './문서/potato.txt'}),\n",
       " Document(page_content='주요 생산국: 중국, 인도, 러시아, 우크라이나, 미국 등이 감자의 주요 생산국입니다.\\n4. 영양 성분 및 건강 효과', metadata={'source': './문서/potato.txt'}),\n",
       " Document(page_content='환경 영향: 감자는 비교적 적은 물과 비료로 재배될 수 있어 환경 친화적인 작물로 평가됩니다.', metadata={'source': './문서/potato.txt'}),\n",
       " Document(page_content='꽃: 감자 식물은 보통 흰색, 분홍색, 보라색, 또는 파란색 꽃을 피웁니다.\\n잎: 잎은 초록색이고, 잎자루를 가지고 있으며, 잎의 가장자리가 물결 모양입니다.\\n3. 재배 및 생산', metadata={'source': './문서/potato.txt'})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke('요약해줘')\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597f0130f7084756b4ea5a08818d16dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the disk and cpu.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "ACCESS_TOKEN = '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'\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "qa = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    "    token = ACCESS_TOKEN\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3439a0969224e1c8ba90c07ebc2539a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "ACCESS_TOKEN = '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'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    # device=-1,\n",
    "    token = ACCESS_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchainhub -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['topic'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], template='{topic}에 대한 간단한 설명'))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_teddynote\n",
      "  Downloading langchain_teddynote-0.0.19-py3-none-any.whl.metadata (550 bytes)\n",
      "Requirement already satisfied: langchain in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from langchain_teddynote) (0.2.6)\n",
      "Collecting kiwipiepy (from langchain_teddynote)\n",
      "  Downloading kiwipiepy-0.18.0-cp311-cp311-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: konlpy in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from langchain_teddynote) (0.6.0)\n",
      "Collecting rank-bm25 (from langchain_teddynote)\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pinecone-client[grpc] (from langchain_teddynote)\n",
      "  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting pinecone-text (from langchain_teddynote)\n",
      "  Downloading pinecone_text-0.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: olefile in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from langchain_teddynote) (0.47)\n",
      "Collecting pdf2image (from langchain_teddynote)\n",
      "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting kiwipiepy-model<0.19,>=0.18 (from kiwipiepy->langchain_teddynote)\n",
      "  Downloading kiwipiepy_model-0.18.0.tar.gz (34.7 MB)\n",
      "     ---------------------------------------- 0.0/34.7 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.5/34.7 MB 17.2 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 2.5/34.7 MB 26.7 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 4.8/34.7 MB 33.8 MB/s eta 0:00:01\n",
      "     ----------- --------------------------- 10.0/34.7 MB 53.1 MB/s eta 0:00:01\n",
      "     ----------------- -------------------- 16.2/34.7 MB 108.8 MB/s eta 0:00:01\n",
      "     ---------------------- --------------- 20.6/34.7 MB 165.0 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 24.6/34.7 MB 93.0 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 25.8/34.7 MB 72.6 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 26.5/34.7 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 27.0/34.7 MB 46.7 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 30.2/34.7 MB 43.5 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 33.7/34.7 MB 43.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  34.0/34.7 MB 34.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  34.7/34.7 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  34.7/34.7 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  34.7/34.7 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------- 34.7/34.7 MB 22.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from kiwipiepy->langchain_teddynote) (1.24.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from kiwipiepy->langchain_teddynote) (4.65.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from konlpy->langchain_teddynote) (1.5.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from konlpy->langchain_teddynote) (4.9.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from langchain->langchain_teddynote) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from langchain->langchain_teddynote) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from langchain->langchain_teddynote) (3.9.3)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from langchain->langchain_teddynote) (0.2.10)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from langchain->langchain_teddynote) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from langchain->langchain_teddynote) (0.1.82)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from langchain->langchain_teddynote) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from langchain->langchain_teddynote) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from langchain->langchain_teddynote) (8.2.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from pdf2image->langchain_teddynote) (10.2.0)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from pinecone-client[grpc]->langchain_teddynote) (2024.2.2)\n",
      "Requirement already satisfied: googleapis-common-protos>=1.53.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from pinecone-client[grpc]->langchain_teddynote) (1.63.2)\n",
      "Requirement already satisfied: grpcio>=1.59.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from pinecone-client[grpc]->langchain_teddynote) (1.64.1)\n",
      "Requirement already satisfied: lz4>=3.1.3 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from pinecone-client[grpc]->langchain_teddynote) (4.3.2)\n",
      "Collecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client[grpc]->langchain_teddynote)\n",
      "  Downloading pinecone_plugin_inference-1.0.3-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client[grpc]->langchain_teddynote)\n",
      "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: protobuf<5.0,>=4.25 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from pinecone-client[grpc]->langchain_teddynote) (4.25.3)\n",
      "Collecting protoc-gen-openapiv2<0.0.2,>=0.0.1 (from pinecone-client[grpc]->langchain_teddynote)\n",
      "  Downloading protoc_gen_openapiv2-0.0.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from pinecone-client[grpc]->langchain_teddynote) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from pinecone-client[grpc]->langchain_teddynote) (2.0.7)\n",
      "Collecting mmh3<5.0.0,>=4.1.0 (from pinecone-text->langchain_teddynote)\n",
      "  Downloading mmh3-4.1.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.6.5 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from pinecone-text->langchain_teddynote) (3.8.1)\n",
      "Collecting python-dotenv<2.0.0,>=1.0.1 (from pinecone-text->langchain_teddynote)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: types-requests<3.0.0,>=2.25.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from pinecone-text->langchain_teddynote) (2.32.0.20240712)\n",
      "Collecting wget<4.0,>=3.2 (from pinecone-text->langchain_teddynote)\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->langchain_teddynote) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->langchain_teddynote) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->langchain_teddynote) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->langchain_teddynote) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->langchain_teddynote) (1.9.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from JPype1>=0.7.0->konlpy->langchain_teddynote) (23.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.10->langchain->langchain_teddynote) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain->langchain_teddynote) (3.10.5)\n",
      "Requirement already satisfied: click in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from nltk<4.0.0,>=3.6.5->pinecone-text->langchain_teddynote) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from nltk<4.0.0,>=3.6.5->pinecone-text->langchain_teddynote) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from nltk<4.0.0,>=3.6.5->pinecone-text->langchain_teddynote) (2023.10.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain->langchain_teddynote) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain->langchain_teddynote) (3.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain->langchain_teddynote) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tqdm->kiwipiepy->langchain_teddynote) (0.4.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain->langchain_teddynote) (2.1)\n",
      "Downloading langchain_teddynote-0.0.19-py3-none-any.whl (32 kB)\n",
      "Downloading kiwipiepy-0.18.0-cp311-cp311-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 77.8 MB/s eta 0:00:00\n",
      "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Downloading pinecone_text-0.9.0-py3-none-any.whl (23 kB)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Downloading mmh3-4.1.0-cp311-cp311-win_amd64.whl (31 kB)\n",
      "Downloading pinecone_plugin_inference-1.0.3-py3-none-any.whl (117 kB)\n",
      "   ---------------------------------------- 0.0/117.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 117.6/117.6 kB ? eta 0:00:00\n",
      "Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
      "Downloading protoc_gen_openapiv2-0.0.1-py3-none-any.whl (7.9 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n",
      "   ---------------------------------------- 0.0/244.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 244.8/244.8 kB 15.6 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: kiwipiepy-model, wget\n",
      "  Building wheel for kiwipiepy-model (setup.py): started\n",
      "  Building wheel for kiwipiepy-model (setup.py): finished with status 'done'\n",
      "  Created wheel for kiwipiepy-model: filename=kiwipiepy_model-0.18.0-py3-none-any.whl size=34843389 sha256=b15182da23ec37030d87400e317a33ee4fede46e9ff2d452032854818c3ba77c\n",
      "  Stored in directory: c:\\users\\smhrd\\appdata\\local\\pip\\cache\\wheels\\1e\\c2\\c6\\68c479859c7e3b08e5644546b403a13d050195e2f21003ee31\n",
      "  Building wheel for wget (setup.py): started\n",
      "  Building wheel for wget (setup.py): finished with status 'done'\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9680 sha256=0883b6f6d0600998e95257ad97cdb4f83b788d3781225c4981bc472842378234\n",
      "  Stored in directory: c:\\users\\smhrd\\appdata\\local\\pip\\cache\\wheels\\40\\b3\\0f\\a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
      "Successfully built kiwipiepy-model wget\n",
      "Installing collected packages: wget, mmh3, kiwipiepy-model, rank-bm25, python-dotenv, pinecone-plugin-interface, pdf2image, protoc-gen-openapiv2, pinecone-plugin-inference, kiwipiepy, pinecone-text, pinecone-client, langchain_teddynote\n",
      "  Attempting uninstall: python-dotenv\n",
      "    Found existing installation: python-dotenv 0.21.0\n",
      "    Uninstalling python-dotenv-0.21.0:\n",
      "      Successfully uninstalled python-dotenv-0.21.0\n",
      "Successfully installed kiwipiepy-0.18.0 kiwipiepy-model-0.18.0 langchain_teddynote-0.0.19 mmh3-4.1.0 pdf2image-1.17.0 pinecone-client-5.0.1 pinecone-plugin-inference-1.0.3 pinecone-plugin-interface-0.0.7 pinecone-text-0.9.0 protoc-gen-openapiv2-0.0.1 python-dotenv-1.0.1 rank-bm25-0.2.2 wget-3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_teddynote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.messages[0].prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = ChatOllama(model='meta-llama-3.1')\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=model,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = conversation_chain({'question': '감자의 종류'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'러셋 감자, 유콘 골드, 레드 포테이토 등이 있습니다'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['chat_history'][1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "감자의 종류\n",
      "러셋 감자, 유콘 골드, 레드 포테이토 등이 있습니다\n"
     ]
    }
   ],
   "source": [
    "for msg in res['chat_history']:\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain\\hub.py:86: DeprecationWarning: The `langchainhub sdk` is deprecated.\n",
      "Please use the `langsmith sdk` instead:\n",
      "  pip install langsmith\n",
      "Use the `pull_prompt` method.\n",
      "  res_dict = client.pull_repo(owner_repo_commit)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': '감자의 종류',\n",
       " 'result': '감자의 종류는 수천 가지가 있으며, 각기 다른 색깔, 크기, 맛을 가지고 있습니다. 대표적인 종류로는 러셋 감자, 유콘 골드, 레드 포테이토 등이 있습니다'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull('rlm/rag-prompt')\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    '''\n",
    "    you are an friendly assistant chatbot for question-answering tasks.\n",
    "    use the follwing pieces of retrieved context to answer the question.\n",
    "    answer me only use korean.\n",
    "    if you can't find the answer, just say '주어진 정보로 대답할 수 없습니다.'\n",
    "    \n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "    Answer:\n",
    "    '''\n",
    ")\n",
    "\n",
    "from langchain.chat_models import ChatOllama\n",
    "model = ChatOllama(model='meta-llama-3.1')\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm = model,\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3}),\n",
    "    chain_type_kwargs={'prompt': prompt}\n",
    ")\n",
    "\n",
    "qa({'query': '감자의 종류'})\n",
    "\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# prompt = ChatPromptTemplate.from_template('{topic}에 대한 간단한 설명')\n",
    "\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# chain = prompt|model|StrOutputParser()\n",
    "\n",
    "# answer = chain.stream({'topic': '딥러닝'})\n",
    "\n",
    "# from langchain_teddynote.messages import stream_response\n",
    "# stream_response(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question, max_new_tokens=512):\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    # print(retrieved_docs)\n",
    "    context = '\\n'.join([docs.page_content +'\\t출처: ' +docs.metadata['source'] for docs in retrieved_docs])\n",
    "    # print(context)\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a friendly chatbot. answer me only use korean. context의 내용만 보고 대답해. 만약 context에 질문에 대한 답이 없다면 '주어진 정보로 대답할 수 없습니다.'라고만 말해\"},\n",
    "        {\"role\": \"context\", \"content\": context},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    print(context)\n",
    "    output = qa(text_inputs = messages, max_new_tokens = max_new_tokens, pad_token_id=qa.tokenizer.eos_token_id)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다양한 종류: 감자의 종류는 수천 가지가 있으며, 각기 다른 색깔, 크기, 맛을 가지고 있습니다. 대표적인 종류로는 러셋 감자, 유콘 골드, 레드 포테이토 등이 있습니다.\t출처: ./문서/potato.txt\n",
      "꽃: 감자 식물은 보통 흰색, 분홍색, 보라색, 또는 파란색 꽃을 피웁니다.\n",
      "잎: 잎은 초록색이고, 잎자루를 가지고 있으며, 잎의 가장자리가 물결 모양입니다.\n",
      "3. 재배 및 생산\t출처: ./문서/potato.txt\n",
      "가족: 가지과 (Solanaceae)\n",
      "형태: 감자는 지하에 있는 덩이줄기(혹은 괴경)가 식용 부분입니다. 이 덩이줄기는 영양분을 저장하는 역할을 합니다.\t출처: ./문서/potato.txt\n",
      "주요 생산국: 중국, 인도, 러시아, 우크라이나, 미국 등이 감자의 주요 생산국입니다.\n",
      "4. 영양 성분 및 건강 효과\t출처: ./문서/potato.txt\n",
      "3. 재배 및 생산\n",
      "재배 조건: 감자는 서늘한 기후를 좋아하며, 최적 성장 온도는 15~20도씨입니다. 배수가 잘 되는 토양이 필요합니다.\t출처: ./문서/potato.txt\n",
      "1. 역사 및 기원\n",
      "기원: 감자는 약 7,000~10,000년 전 페루와 볼리비아의 안데스 산맥 지역에서 처음 재배되었습니다.\t출처: ./문서/potato.txt\n",
      "감자는 전 세계적으로 사랑받는 식품으로, 다양한 요리에 활용될 수 있으며, 건강에도 좋은 영향을 미치는 중요한 식량 자원입니다.\t출처: ./문서/potato.txt\n",
      "건강 효과:\n",
      "에너지 공급: 감자는 탄수화물이 풍부해 에너지 공급원으로 좋습니다.\n",
      "소화 개선: 식이섬유가 풍부해 소화 건강에 도움이 됩니다.\t출처: ./문서/potato.txt\n",
      "5. 요리 및 활용\n",
      "요리법: 감자는 다양하게 요리될 수 있습니다. 감자튀김, 매시드 포테이토, 감자 샐러드, 구운 감자 등 여러 가지 요리법이 있습니다.\t출처: ./문서/potato.txt\n",
      "4. 영양 성분 및 건강 효과\n",
      "영양 성분: 감자는 비타민 C, 비타민 B6, 칼륨, 철분, 식이섬유 등이 풍부합니다. 감자는 칼로리가 낮고, 지방이 거의 없습니다.\n",
      "건강 효과:\t출처: ./문서/potato.txt\n"
     ]
    }
   ],
   "source": [
    "answer = ask('감자의 종류')\n",
    "# answer[0]['generated_text'][-1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m context\n",
      "\u001b[1;31mNameError\u001b[0m: name 'context' is not defined"
     ]
    }
   ],
   "source": [
    "context\n",
    "\n",
    "# 사랑해요 박 건\n",
    "# 건 씨도 우리팀\n",
    "# 야호 우리팀\n",
    "# 우리 코드 대신 짜줘요\n",
    "# 제발요\n",
    "# 부탁임"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
