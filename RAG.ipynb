{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.ncloud-forums.com/topic/277 <br>\n",
    "https://velog.io/@kwon0koang/%EB%A1%9C%EC%BB%AC%EC%97%90%EC%84%9C-Llama3-%EB%8F%8C%EB%A6%AC%EA%B8%B0#-rag <br>\n",
    "RAG에 대한 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\SMHRD\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs = {'device': 'cpu'}, # 모델이 CPU에서 실행되도록 설정. GPU를 사용할 수 있는 환경이라면 'cuda'로 설정할 수도 있음\n",
    "    encode_kwargs = {'normalize_embeddings': True}, # 임베딩 정규화. 모든 벡터가 같은 범위의 값을 갖도록 함. 유사도 계산 시 일관성을 높여줌\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "doclist = [\n",
    "    './문서/potato.txt',\n",
    "    './문서/sweetpotato.txt',\n",
    "]\n",
    "data = []\n",
    "\n",
    "for doc in doclist:\n",
    "    loader = TextLoader(doc)\n",
    "    docdata = loader.load()\n",
    "    data += docdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://wikidocs.net/234014 <br>\n",
    "FAISS에 대한 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    splits,\n",
    "    embedding = embedding_model,\n",
    ")\n",
    "\n",
    "MY_FAISS_INDEX = \"MY_FAISS_INDEX\"\n",
    "vectorstore.save_local(MY_FAISS_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "MY_FAISS_INDEX = \"MY_FAISS_INDEX\"\n",
    "\n",
    "vectorstore = FAISS.load_local(\n",
    "    MY_FAISS_INDEX, \n",
    "    embedding_model, \n",
    "    allow_dangerous_deserialization=True #잠재적으로 위험한 데이터 구조나 객체를 포함할 수 있는 인덱스 파일의 로딩을 허용\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10}) # 유사도 높은 10문장 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "ACCESS_TOKEN = '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'\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "qa = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    "    token = ACCESS_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question, max_new_tokens=512):\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    # print(retrieved_docs)\n",
    "    context = '\\n'.join([docs.page_content +'\\t출처: ' +docs.metadata['source'] for docs in retrieved_docs])\n",
    "    # print(context)\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a friendly chatbot. answer me only use korean. context의 내용만 보고 대답해. 만약 context에 질문에 대한 답이 없다면 '주어진 정보로 대답할 수 없습니다.'라고만 말해\"},\n",
    "        {\"role\": \"context\", \"content\": context},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    output = qa(messages, max_new_tokens = max_new_tokens, pad_token_id=qa.tokenizer.eos_token_id)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = ask('감자의 종류')\n",
    "answer[0]['generated_text'][-1]['content']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
